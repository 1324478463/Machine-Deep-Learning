## 关于正则化

batchnorm计算的是不同通道下本batch的所有样本的均值方差。

那输入应该是什么？对于一个batch，X([N, C, H, W])，应该输入什么？

spring1718_assignment2_v2/BatchNormalization.ipynb中的这部分，以随机样本进行计算，如果对于图像而言，真正的输入应该是什么样子的？

```python
np.random.seed(231)
# 可以理解为200张50*60*3的照片集合
N, D1, D2, D3 = 200, 50, 60, 3
# 输入的数据被归置到前两维度上面，这里怎么理解？对于一副真实图像，如何理解此处对应的X
X = np.random.randn(N, D1)
W1 = np.random.randn(D1, D2)
W2 = np.random.randn(D2, D3)
# affine-relu-affine
a = np.maximum(0, X.dot(W1)).dot(W2)

print('Before batch normalization:')
print_mean_std(a,axis=0)

# 这里为什么要使用D3来初始化长度，可以从后面计算的过程中看出来，gamma也是针对各个特征有一个gamma，beta
gamma = np.ones((D3,))
beta = np.zeros((D3,))

# Means should be close to zero and stds close to one
print('After batch normalization (gamma=1, beta=0)')
a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})
print_mean_std(a_norm,axis=0)

gamma = np.asarray([1.0, 2.0, 3.0])
beta = np.asarray([11.0, 12.0, 13.0])
# Now means should be close to beta and stds close to gamma
# 重构之后，得到的均值标准差和theta,gamma相近
print('After batch normalization (gamma=', gamma, ', beta=', beta, ')')
a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})
print_mean_std(a_norm,axis=0)
```

在layers.py文件中的batchnorm_forward函数中，有这样的代码：

```python
mode = bn_param['mode']                                                   
eps = bn_param.get('eps', 1e-5)                                           
momentum = bn_param.get('momentum', 0.9)                                  

N, D = x.shape                                                            
running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))   
running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))     

out, cache = None, None

...

# 均值的计算，是计算N方向上的，也就是算的是所有样本对于同一特征的均值
sample_mean = np.mean(x, axis=0)
# 计算标准差（这之中使用了参数epsilon)
sample_var = np.var(x, axis=0)                                 
# 动量方式滑动更新策略
running_mean = momentum * running_mean + (1 - momentum) * sample_mean  
running_var = momentum * running_var + (1 - momentum) * sample_var     
# 计算归一化值                                                         
x_normed = (x - sample_mean) / np.sqrt(sample_var + eps)               
# 变换重构                                                             
out = gamma * x_normed + beta                                          
# 保存参数                                                             
cache = (x, gamma, beta, x_normed, sample_mean, sample_var, eps)       
```

从上面的过程来看，应该是输入成二维的数据，对于**批量的三维图像**而言，可能会有转化为 [N,(H×W×C)]的一个二维输入。看下后面在类里的使用方式：

```python
np.random.seed(231)
N, D, H1, H2, C = 2, 15, 20, 30, 10
X = np.random.randn(N, D)
y = np.random.randint(C, size=(N,))

# You should expect losses between 1e-4~1e-10 for W, 
# losses between 1e-08~1e-10 for b,
# and losses between 1e-08~1e-09 for beta and gammas.
for reg in [0, 3.14]:
    print('Running check with reg = ', reg)
    model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,
                              reg=reg, weight_scale=5e-2, dtype=np.float64,
                              normalization='batchnorm')

    loss, grads = model.loss(X, y)
    print('Initial loss: ', loss)

    for name in sorted(grads):
        f = lambda _: model.loss(X, y)[0]
        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)
        print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))
    if reg == 0: print()
```

这里涉及到BN的用法和上面一样。

为了找到更实际的，找到了这部分内容：

```python
np.random.seed(231)
# Try training a very deep net with batchnorm
hidden_dims = [100, 100, 100, 100, 100]

num_train = 1000
# data：
# X_train:  (49000, 3, 32, 32)
# y_train:  (49000,)
# X_val:  (1000, 3, 32, 32)
# y_val:  (1000,)
# X_test:  (1000, 3, 32, 32)
# y_test:  (1000,)
small_data = {
  'X_train': data['X_train'][:num_train],
  'y_train': data['y_train'][:num_train],
  'X_val': data['X_val'],
  'y_val': data['y_val'],
}
# X_train: (1000, 2, 32, 32)
# y_train: (1000, )
# ...

weight_scale = 2e-2

bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization='batchnorm')
bn_solver = Solver(bn_model, small_data,
                num_epochs=10, batch_size=50,
                update_rule='adam',
                optim_config={
                  	'learning_rate': 1e-3,
                },
                verbose=True,print_every=20)
bn_solver.train()

model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, normalization=None)
solver = Solver(model, small_data,
                num_epochs=10, batch_size=50,
                update_rule='adam',
                optim_config={
                  	'learning_rate': 1e-3,
                },
                verbose=True, print_every=20)
solver.train()
```

查看Solver类对应的model.loss()向调用的函数查找，找到了：

fc_net.py中的全连接类的

![1537407981290](assets/1537407981290.png)

```python
def loss(self, X, y=None):                                                                         
    """                                                                                       
    Compute loss and gradient for a minibatch of data.                                        
    Inputs:                                                   
    - X: Array of input data of shape (N, d_1, ..., d_k)      
    - y: Array of labels, of shape (N,). y[i] gives the label for X[i].    
                                                                          
    Returns:                                                            
    If y is None, then run a test-time forward pass of the model and return:   
    - scores: Array of shape (N, C) giving classification scores, where        
      scores[i, c] is the classification score for X[i] and class c.     
    If y is not None, then run a training-time forward and backward pass and   
    return a tuple of:                                                         
    - loss: Scalar value giving the loss                              
    - grads: Dictionary with the same keys as self.params, mapping parameter    
      names to gradients of the loss with respect to those parameters.         
    """                                                                                            
    scores = None                                                      
    ############################################################################     
    # TODO: Implement the forward pass for the two-layer net, computing the    #  
    # class scores for X and storing them in the scores variable.              #  
    ############################################################################  
    # affine_1 W1 input*hidden             
    #  affine_1_out = X.dot(self.params['W1']) + self.params['b1']     
    # relu_1                                       
    #  relu_1_out = np.maximum(0, affine_1_out)   
    # affine_2 W2 hidden*classes                                                               
    #  affine_2_out = relu_1_out.dot(self.params['W2']) + self.params['b2']                 
    # 最后的softmax不计算，因为得分的计算计算到最后的分类之前即可                                  
    #  scores = affine_2_out    
    # 思路是上面的思路，但是实际计算的时候需要注意具体的数据形式，这里还是用已有                   
    # 的函数计算比较方便                                                                           
    layer_1, cache_1 = affine_relu_forward(     
        X, self.params['W1'], self.params['b1'])   
    layer_2, cache_2 = affine_forward(   
        layer_1, self.params['W2'], self.params['b2'])    
    scores = layer_2                                                                           
```
再看affine_relu_forward中调用了affine_forward，找到了：

```python
N = x.shape[0]
# 将多维e的x归置到二维，各个样本的数据都被压缩到一个维度
new_x = x.reshape([N, -1])
out = np.dot(new_x, w) + b
```
原来，对于batchnorm而言，得到的batch数据x是二维数据，N×D，D=H×W×C，所以再看batchnorm_forward函数：

```python
sample_mean = np.mean(x, axis=0)
# 计算标准差（这之中使用了参数epsilon)
sample_var = np.var(x, axis=0)
# 动量方式滑动更新策略
running_mean = momentum * running_mean + (1 - momentum) * sample_mean
running_var = momentum * running_var + (1 - momentum) * sample_var
# 计算归一化值
x_normed = (x - sample_mean) / np.sqrt(sample_var + eps)
# 变换重构
out = gamma * x_normed + beta
# 保存参数
cache = (x, gamma, beta, x_normed, sample_mean, sample_var, eps)
```
得到的均值方差都是各个特征的对应的均值方差。

也就实际上是说，对于batchnorm而言，实际上是对一个batch而言，求得各个特征的均值方差，对各个特征进行归一化。

而对于layernorm而言，实际上略有不同：

```python
# 因为layernorm是关于特征的处理，x到这里的时候，已经是二维的了。
x_new = x.copy()                                               
# 这里的计算均值方差，是针对一个样本而言                       
sample_mean = np.mean(x_new, axis=1)                           
sample_var = np.var(x_new, axis=1)                             
                                                               
# 这里是按照不同的样本进行的均值方差计算                       
x_new = x_new - sample_mean.reshape([-1, 1])                   
x_new = x_new / np.sqrt(sample_var.reshape([-1, 1]) + eps)     
out = gamma * x_new + beta                                     
cache = (x, gamma, beta, x_new, sample_mean, sample_var, eps)  
```
可见这里的均值方差与batchnorm计算方向不同，不过这里也使用了batch的方式来计算，只是计算方向不同，所以与N无关，而batchnorm与N紧密关联。

而对于spatial_batchnorm_forward而言，它的输入是四维的batch，它是把四维里的除了C维度的三维压缩到一起。

```python
N, C, H, W = x.shape
# 调整x到 N×H×W × C 大小，四维压缩到二维，保留C维度
# np.transpose可以调整各维度的顺序，对于二维而言也就是转置了
x_new = np.reshape(np.transpose(x, (0, 2, 3, 1)), (-1, C))

out, cache = batchnorm_forward(x_new, gamma, beta, bn_param)

# 调整out从(N, H, W, C)到(N, C, H, W)
out = np.transpose(np.reshape(out, (N, H, W, C)), (0, 3, 1, 2))
```
SBN运算相当于它是在C方向上划分了数据，对于图像而言，就是划分了三层，RGB各一层。再进行BN运算。

原始的BN运算，是在N方向上划分数据，对于图像batch而言，就是划分了多个样本，一个样本一层。

对于作业中出现的spatial_groupnorm_forward而言，它的输入是四维的，但是在压缩调整的时候，在C的方向上进行了分组，分成G组，一组也看做一个样本，从而有N×G个样本，再进行BN。

> C方向一般不是指代通道数目方向么？那一般不就是3么？

```python
N, C, H, W = x.shape
# GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)*H*W的均值
x = np.reshape(x, (N * G, C // G * H * W))

x = x.T
mu = np.mean(x, axis=0)
xmu = x - mu
sq = xmu ** 2
var = np.var(x, axis=0)

sqrtvar = np.sqrt(var + eps)
ivar = 1. / sqrtvar
xhat = xmu * ivar

xhat = np.reshape(xhat.T, (N, C, H, W))
out = gamma[np.newaxis, :, np.newaxis, np.newaxis] * \
    xhat + beta[np.newaxis, :, np.newaxis, np.newaxis]
cache = (xhat, gamma, xmu, ivar, sqrtvar, var, eps, G)
```