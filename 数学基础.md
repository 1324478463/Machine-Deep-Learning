

## p范数

A class of [vector norms](http://planetmath.org/vectornorm), called a p-norm and denoted $||⋅||_p$, is defined as

$||x||_p=(|x_1|^p+⋯+|x_n|^p)^{1/p} ,p≥1,x∈R^n$

The most widely used are the 1-norm, 2-norm, and ∞∞-norm:

$||x||_1=(|x_1|^1+⋯+|x_n|^1)^{1} $
$||x||_2=\sqrt{(|x_1|^2+⋯+|x_n|^2)^{2} }$
$||x||_∞== max_{1≤i≤n}|x_i|$

## 表示定理（Representer Theorem）

https://www.analyticsindiamag.com/what-is-representer-theorem-in-machine-learning/

## 支持向量机

http://blog.pluskid.org/?page_id=683

http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf

## 记号规则

$log=ln$

## 次梯度

设f在实数域上是一个凸函数，定义在数轴上的开区间内。这种函数不一定是处处可导的，例如绝对值函数f(x)=|x|f(x)=|x| 。对于下图来说，对于定义域中的任何x0，我们总可以作出一条直线，它通过点(x0, f(x0))，并且要么接触f的图像，要么在它的下方。直线的斜率称为函数的次导数。次导数的集合称为函数f在x0处的次微分。

Rigorously, a *subderivative* of a convex function *f*:*I*→**R** at a point *x*0 in the open interval *I* is a real number *c* such that

![f(assets/f95b377f545e11b41622f7f1db9dfc8d7666056e)-f(x_{0})\geq c(x-x_{0})](https://wikimedia.org/api/rest_v1/media/math/render/svg/f95b377f545e11b41622f7f1db9dfc8d7666056e)

for all *x* in *I*. One may show that the [set](https://en.wikipedia.org/wiki/Set_(mathematics)) of subderivatives at *x*0 for a convex function is a [nonempty](https://en.wikipedia.org/wiki/Empty_set) [closed interval](https://en.wikipedia.org/wiki/Closed_interval) [*a*, *b*], where *a* and *b* are the [one-sided limits](https://en.wikipedia.org/wiki/One-sided_limit)

对于所有x，我们可以证明**在点$x_0$ 的次导数的集合是一个非空闭区间$[a,b]$**，其中a和b是单测极限一定存在，且$a<=b$，在$[a,b]$内的所有次导数是f在$x_0$的次微分。

![a=\lim _{{x\to x_{0}^{-}}}{\frac  {f(x)-f(x_{0})}{x-x_{0}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/c73352f25b3e8547a685a1b7b5f60e833519eb62)

![b=\lim _{{x\to x_{0}^{+}}}{\frac  {f(x)-f(x_{0})}{x-x_{0}}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/cf7130df8ba8fcf200258555c8e095948e8f4413)

which are guaranteed to exist and satisfy *a* ≤ *b*.

The set [*a*, *b*] of all subderivatives is called the **subdifferential** of the function *f* at *x*0. Since *f* is convex, if its subdifferential at $x_{0}$ contains exactly one subderivative, then *f* is differentiable at $x_{0}$.

当函数在x0处可导时，次微分只有一个点组成，这个点就是函数在x0处的导数。

## 激活函数

![blog.csdn.net_u013146742_article_details_51986575](assets/blog.csdn.net_u013146742_article_details_51986575.png)

## 为什么样本方差（sample variance）的分母是 n-1？

https://www.zhihu.com/question/20099757

## 梯度下降与梯度上升

通常，您使用梯度上升来最大化似然函数，并使用梯度下降来最小化成本函数。梯度下降和上升几乎都是相同的。让我举一个具体的例子，使用一个简单的基于梯度的优化友好算法和一个凹/凸 似然/成本函数：逻辑回归。

您希望在逻辑回归中最大化的似然函数是![在此处输入图像描述](https://i.stack.imgur.com/vRpFZ.png).

其中“phi”只是sigmoid函数![在此处输入图像描述](https://i.stack.imgur.com/BGoE9.png)

现在，你想要一个用于梯度上升的concav函数：

![在此处输入图像描述](https://i.stack.imgur.com/VCrcH.png)

类似地，您可以将其写为反向，以获得可通过梯度下降最小化的成本函数。

![在此处输入图像描述](https://i.stack.imgur.com/hj0WT.png)

对于对数似然，您将导出并应用梯度上升，如下所示：

![在此处输入图像描述](https://i.stack.imgur.com/6lpk9.png)

![在此处输入图像描述](https://i.stack.imgur.com/SPD75.png)

既然你想同时更新所有权重，那就让我们把它写成

![在此处输入图像描述](https://i.stack.imgur.com/x6Hn6.png)

现在，应该很明显看到梯度下降更新与上升更新相同，但请记住，我们正在将其制定为“向成本函数的梯度的相反方向迈出一步”

![在此处输入图像描述](https://i.stack.imgur.com/j9xHh.png)

## 矩阵乘法

### Hadamard product(哈达马乘积)

`m x n`矩阵`A = [aij]`与矩阵`B = [bij]`的Hadamard积，记为`A * B` 。新矩阵元素定义为矩阵A、B**对应元素的乘积**`(A * B)ij = aij.bij`。

![{\displaystyle (A\circ B)_{i,j}=(A)_{i,j}(B)_{i,j}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0c463ca925b4af07b1ac93b1c9a56d562ba1c2dc)

![{\displaystyle \left[{\begin{array}{ccc}\mathrm {a} _{11}&\mathrm {a} _{12}&\mathrm {a} _{13}\\\mathrm {a} _{21}&\mathrm {a} _{22}&\mathrm {a} _{23}\\\mathrm {a} _{31}&\mathrm {a} _{32}&\mathrm {a} _{33}\end{array}}\right]\circ \left[{\begin{array}{ccc}\mathrm {b} _{11}&\mathrm {b} _{12}&\mathrm {b} _{13}\\\mathrm {b} _{21}&\mathrm {b} _{22}&\mathrm {b} _{23}\\\mathrm {b} _{31}&\mathrm {b} _{32}&\mathrm {b} _{33}\end{array}}\right]=\left[{\begin{array}{ccc}\mathrm {a} _{11}\,\mathrm {b} _{11}&\mathrm {a} _{12}\,\mathrm {b} _{12}&\mathrm {a} _{13}\,\mathrm {b} _{13}\\\mathrm {a} _{21}\,\mathrm {b} _{21}&\mathrm {a} _{22}\,\mathrm {b} _{22}&\mathrm {a} _{23}\,\mathrm {b} _{23}\\\mathrm {a} _{31}\,\mathrm {b} _{31}&\mathrm {a} _{32}\,\mathrm {b} _{32}&\mathrm {a} _{33}\,\mathrm {b} _{33}\end{array}}\right]}](https://wikimedia.org/api/rest_v1/media/math/render/svg/06e3f6abf1511656029ce58b89695b687789aa9c)



## Kronecker product(克罗内克积)

Kronecker积是两个任意大小矩阵间的运算，表示为 `A x B`。如果A是一个 `m x n` 的矩阵，而B是一个 `p x q` 的矩阵，克罗内克积则是一个 `mp x nq` 的矩阵。克罗内克积也称为直积或张量积，以德国数学家利奥波德·克罗内克命名。![image](https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike116%2C5%2C5%2C116%2C38/sign=7dde9e4076c6a7efad2ba0749c93c434/d009b3de9c82d1585670622e800a19d8bd3e42ba.jpg)

### 斯特拉森矩阵乘法

斯特拉森矩阵乘法是1969年斯特拉森利用分治策略并加上一些处理技巧设计出的一种矩阵乘法。设A和B是俩个`n x n`的[矩阵](https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5/18069)，其中n可以写成2的幂。将A和B分别等分成4个小矩阵，此时如果把A和B都当成2x2矩阵来看,每个元素就是一个`（n/2）x（n/2）`矩阵，而A和B的成积就可以写成：

![1538276233903](assets/1538276233903.png)

![1538276247041](assets/1538276247041.png)

![1538276312957](assets/1538276312957.png)

用上述方案解n=2;矩阵乘法；假定施特拉斯矩阵分割方案仅用于n>=8的矩阵乘法，而对于小于8的矩阵直接利用公式计算；n的值越大，斯拉特森方法更方便.