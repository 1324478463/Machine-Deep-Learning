# 卷积

1. 卷积特征提取操作的时候，卷积核的得出是通过具有一定数目隐含单元单元的自编码网络来进行训练得到的，因为卷积核实际上就是一个映射参数的集合。所以计算“卷积”的时候，就是”.*”运算。

   1. 若输入8X8图像，实际上输入为64维向量。
   2. 隐含层为100个单元，则对应的W^(1)^ 大小为100X64。
   3. 隐含层输出为100维特征向量。
   4. 则对应的，如果8X8是96X96图像分割后的一部分，则最后是有89X89个100维的特征向量。
   5. 实际上组成了100个89X89大小的特征矩阵，该矩阵就是卷积特征矩阵。
   6. 卷积的时候，一个8X8大小的卷积核，扫描全部图像，获得的是对应于该隐含单元（针对这一个特征）的一个卷积特征矩阵。这里是89X89大小的。

2. 池化作用于获得的特征矩阵。

3. 该矩阵实际上和神经网络前两层的功能是类似的，实际上是利用部分联通实现了全联通。一个核对应一个特征（隐含单元激活值）。扫描检测图像上不同位置该特征的激活状态，也就是该特征的存在情况，不同特征的总体存在情况进行汇总，就反映出来整个图像。

4. 多个卷积层可以级联，实现更高维特征的检测。

5. 卷积核实际上是三维的，但是扫描是在自己高度的空间里扫描，其余高度的扫描算是另一个核的扫描结果，属于另一张卷积特征图。 

   ---

   第一步，针对一个神经元，一幅640X360图像，一个神经元要对应640X360个像素点，即一个神经元对应全局图像，全连接的话一个神经元就有640X360个参数；

   第二步，然而，图像的空间联系是局部的，就像人是通过一个局部的感受野去感受外界图像一样，每一个神经元都不需要对全局图像做感受，每个神经元只感受局部的图像区域，然后在更高层，将这些不同局部的神经元综合起来就可以得到全局信息。假如每个局部感受野10X10，每个局部感受野只需要和10X10的局部图像连接，这样一个神经元就只需要10X10个参数；

   第三步，全局图像是640X360，但局部图像只有10X10大小，10X10个参数只针对局部图像，如果全局图像中各个局部图像之间权值共享的话，即10X10个参数在不同局部图像上参数应用相同的话，则在全局图像上通过全局共享则只需要10X10个参数；

   第四步，10X10个参数只针对一个神经元，要是有100万个神经元，则需要100万X10X10个参数，神经元多后，参数还是太大，如果每个神经元的这10X10个参数相同呢，这样就还是只需要10X10参数，因而经过局部感受野到权值共享再到每个神经元的10X10个参数相同，不管图像多大，不管每层神经元个数多少，而两层间连接还是只需要求解10X10个参数；

   第五步，由于只有一个滤波器，只提取了一种特征，特征也太少了。一种滤波器也就是一种卷积核就是提取图像一种特征，例如某个方向的边缘。那么我们需要提取不同特征怎么办，多加几个滤波器不就行了。假设我们加到100种滤波器，每种滤波器的参数不一样，表示提取输入图像不同特征，例如不同边缘。这样不同滤波器去卷积图像就得到不同特征的放映，我们称之为Feature Map，所以100中卷积核就有100个Feature Map，这100个Feature Map就组成了一层神经元。我们这一层有多少个参数到这时候就明了吧，100种卷积核，每种卷积核100个参数  = 100 * 100 = 10000个参数。

   最后，刚才说每一个隐藏层的参数个数和隐藏层的神经元个数无关，只和滤波器大小和滤波器种类数有关，那么隐藏层的神经元个数怎么确定呢？它和原图像，也就是输入的大小（神经元个数）、滤波器的大小和滤波器在图像中的滑动步长都有关！假如我的图像是1000X1000像素的，而滤波器大小是10X10，假设步长为10，即滤波器没有重叠，这样隐藏层的神经元个数就是1000 X1000 / (10X10) = 100X100个神经元（如果步长为8，卷积核会重叠2个像素）。这只是一种滤波器，也就是一个Feature Map的神经元个数哦，如果100个Feature Map就是100倍了，   需要注意一点，上面的讨论都没有考虑每个神经元的偏置部分，所以权值个数需要加1，这也是同一种滤波器共享。如滤波器10X10，卷积核个数6，则参数个数为: (10*10 +1) * 6 = 606.