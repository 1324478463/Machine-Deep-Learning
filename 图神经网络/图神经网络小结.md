# 图神经网络小结

简单的图的参数介绍: $\mathcal { G } = ( V , E , A )$

* V(N个)是节点集合, 只描述了存在的节点
* E是边集合, 只描述了存在的连接(边)
* A(NxN)是图的邻接矩阵, 描述了节点之间的连接关系, 当对应的边eij(连接vi,vj的边)存在的时候, 则对应的Aij = wij > 0, 若eij不存在, 则Aij = 0
* D(NxN)是图的度矩阵, 描述了节点自身连接的边的数目, 是对角矩阵, $D_{ii} = ( \sum A_i)$
* X(NxF)是节点属性矩阵, 每个节点对应拥有一个长为F的属性向量, 图可以用节点属性来关联
* L(NxN)是图的拉普拉斯矩阵, 归一化的拉普拉斯矩阵是图的一个鲁棒的数学表示. $L=D-A, L_n=I_n-D^{-1/2}AD^{-1/2}$, 归一化图拉普拉斯矩阵具有实对称半正定的性质, 也就是可以进行特征分解得到特征值和特征向量.

## 图神经网络分类

图网络有很多结构, 这里按照最新的综述论文 A Comprehensive Survey on Graph Neural Networks 文中的分类方式, 主要分为:

* graph convolution networks
* graph attention networks
* graph auto-encoders
* graph generative networks
* graph spatial-temporal networks

这里主要介绍图卷积网络GCN. 因为GCN在捕获结构化依赖上扮演着中心角色.

## GCN: 由谱方法到空域方法

Inspired by the huge success of convolutional networks in the computer vision domain, a large number of methods that re-define the notation of convolutionfor graph data have emerged recently.

These approaches are under the umbrella of  graph  convolutional  networks  (GCNs). The  first  prominent research on GCNs is presented in Bruna et al. (2013), which  develops a variant  of  graph  convolution  based  on spectral graph theory [20]. Since that time, there have been increasing  improvements,  extensions,  and  approximations on  **spectral-based  graph  convolutional  networks**  [12],  [14],[21],  [22],  [23].

As  spectral  methods  usually  handle  the whole  graph  simultaneously  and  are  difficult  to  parallel or scale to large graphs, **spatial-based graph convolutional networks**  have  rapidly  developed  recently  [24],  [25],  [26],[27]. These methods directly *perform the convolution in the graph domain by aggregating the neighbor nodes' information*. Together with sampling strategies, the computation can be performed in a batch of nodes instead of the whole graph[24], [27], which has the potential to improve the efficiency.

## GCN概述

将应用于传统数据的卷积操作, 泛化到了图数据上面. 关键是学习一个函数f, 来通过集成自己和邻居的特征生成节点的表示. GCN在构建其他复杂的图神经网络的任务上, 扮演着重要的角色. 包括auto-encoder-based models, generative models, spatial-temporal networks等.

对于GCN而言, 在集成邻居的过程的时候, 使用的权重是无参数的, 而GAN(图注意力网络)使用了一个端到端神经网络架构捕获的权重, 来使得更重要的节点拥有更大的权重.(这里更多的是表述空域的GCN)

对于图卷积网络, 其实是在*尝试通过图谱理论或者局部空间来定义图卷积*的方式来在图数据上复制CNN的成功.

### GCN的输出机制

使用图结构和节点内容信息作为输入, GCN的输出使用以下不同的机制, 可以关注于不同的图分析任务.

1. 节点级输出与节点回归和分类任务相关. 图卷积模块直接给出节点潜在的表达, 多层感知机或者softmax层被用作最终的GCN层.
2. 边级输出与边分类和连接预测任务相关. 为了预测边的标记和连接强度, 一个附加函数将会把来自图卷积模块的两个节点的潜在表达作为输入.
3. 图级输出与图分类任务有关. 为了获取一个图级的紧凑表达, 池化模块被用来粗化图到子图, 或用来加和/平均节点表达.

这里有一个总结的表, 总结了主要方法的情况:

![img](assets/2019-03-01-11-43-46.png)

对于图卷积网络, 是可以通过半监督/全监督/无监督的方式进行端到端训练的, 注意取决于学习任务和标签信息的获取.

### GCN的不同方法

* 谱方法通过从图信号处理的视角引入滤波器来定义图卷积, 这里图卷积操作被解释为移除图信号中的噪声
* 空域方法将图卷积表达为从邻域中集成特征信息的过程.
* 当GCN在节点等级上操作的时候, **图池化模块**, 可以超入到GCN层中, 来粗化图到高级的子图. 这样一个结构被用来提取图级表达和处理图分类任务.

### 基于谱方法的GCN

#### 初始

基于图信号处理的知识, 图上的卷积可以被定义为: $\mathbf { x } * _ { G } \mathbf { g } _ { \theta } = \mathbf { U } \mathbf { g } _ { \theta } \mathbf { U } ^ { T } \mathbf { x }$

其中$\mathbf { g } _ { \theta } = \operatorname { diag } \left( \mathbf { U } ^ { T } \mathbf { g } \right)$表示滤波器. 可以看做是L特征值的函数, 也就是 $g _ { \theta } ( \Lambda )$.

这里有$\mathbf { x } \in \mathbf { R } ^ { N }$, xi表示第i个节点的值(这里相当于节点的特征向量长度只有1).

这个等式计算消耗大, **计算L的特征分解对于大图来说可能是非常昂贵**的.

在后续的改进中, 这里的信号的设定被进一步扩展, 输入信号表示为: $\mathbf { X } ^ { k } \in \mathbf { R } ^ { N \times f _ { k - 1 } }$, 这里$f_k-1$表示输入通道数, $f_k$表示输出通道数. 滤波器也整合表示为$\mathbf { g } _ { \theta } = \Theta _ { i , j } ^ { k }$, 表示为:

$\mathbf { X } _ { : , j } ^ { k + 1 } = \sigma \left( \sum _ { i = 1 } ^ { f _ { k - 1 } } \mathbf { U } \Theta _ { i , j } ^ { k } \mathbf { U } ^ { T } \mathbf { X } _ { \mathbf { s } , i } ^ { k } \right) \quad \left( j = 1,2 , \cdots , f _ { k } \right)$

这里的 $\sigma$ 表示的是一个非线性转换.

#### 切比雪夫K阶截断: ChebNet

之后有人利用切比雪夫多项式来近似表示了滤波器, 进一步简化了公式的计算:

$\mathbf { g } _ { \theta } = \sum _ { i = 1 } ^ { K } \theta _ { i } T _ { k } ( \tilde { \boldsymbol { \Lambda } } ) ,$ where $\tilde { \boldsymbol { \Lambda } } = 2 \Lambda / \lambda _ { \max } - \mathbf { I } _ { \mathrm { N } }$

> Chebyshev polynomials are defined recursively by $T _ { k } ( x ) = 2 x T _ { k - 1 } ( x ) - T _ { k - 2 } ( x )$ with $T _ { 0 } ( x ) = 1$ and $T _ { 1 } ( x ) = x$

因此卷积操作进一步表示为:

$\begin{aligned} \mathbf { x } * _ { G } \mathbf { g } _ { \theta } & = \mathbf { U } \left( \sum _ { i = 1 } ^ { K } \theta _ { i } T _ { k } ( \tilde { \boldsymbol { \Lambda } } ) \right) \mathbf { U } ^ { T } \mathbf { x } \\ & = \sum _ { i = 1 } ^ { K } \theta _ { i } T _ { i } ( \tilde { \mathbf { L } } ) \mathbf { x } \\ \text { where } \tilde { \mathbf { L } } = 2 \mathbf { L } / \lambda _ { \max } - \mathbf { I } _ { \mathrm { N } } \end{aligned}$

这个时候, 可以看这个表达式, 这个卷积的计算只依赖于中心节点的K阶邻居.(K步以内可达的节点). 而且这样定义的滤波器也是稀疏的.

#### 一阶ChebNet

当前面的公式中, K=1的时候, 并且使得 $\lambda_{max}=2$, 则进一步简化为:

$\mathbf { x } * _ { G } \mathbf { g } _ { \theta } = \theta _ { 0 } \mathbf { x } - \theta _ { 1 } \mathbf { D } ^ { - \frac { 1 } { 2 } } \mathbf { A } \mathbf { D } ^ { - \frac { 1 } { 2 } } \mathbf { x }$

为了进一步减少参数, 以避免过拟合, 这里进行了进一步简化, 让两个 $\theta$ 相反. 也就是$\theta = \theta _ { 0 } = - \theta _ { 1 }$, 这就得到了最终简化后的定义:

$\mathbf { x } * _ { G } \mathbf { g } _ { \theta } = \theta \left( \mathbf { I } _ { \mathbf { n } } + \mathbf { D } ^ { - \frac { 1 } { 2 } } \mathbf { A } \mathbf { D } ^ { - \frac { 1 } { 2 } } \right) \mathbf { x }$

这里最终表示为:

$\mathbf { X } ^ { \mathbf { k } + \mathbf { 1 } } = \tilde { \mathbf { A } } \mathbf { X } ^ { \mathbf { k } } \Theta$

where $\tilde { \mathbf { A } } = \mathbf { I } _ { \mathbf { N } } + \mathbf { D } ^ { - \frac { 1 } { 2 } } \mathbf { A } \mathbf { D } ^ { - \frac { 1 } { 2 } }$

通过一阶ChebNet定义的图卷积在空间上是局部的, 这弥补了谱方法与空域方法的差别. 输出的每行表示每个节点通过一个加权集成了自身和相邻节点的信息了的线性转换来获得的潜在表征.

然而, 一阶ChebNet主要的问题是, 在批量训练过程中，随着1stChebNet层数的增加，计算代价呈指数增长. 最后一层中的每个节点必须在以前的层中递归地扩展其邻域. 一些工作已这对这些问题进行了改进.

#### 自适应图卷积网络AGCN

为了探索没有被图拉普拉斯矩阵指定的隐藏结构关系, 有人提出了AGCN, AGCN用一个所谓的残差图来扩充一个图，这个图是通过计算节点之间的成对距离来构造的,  尽管能够捕获互补的关系信息, 但是它的计算复杂度为$O(N^2)$.

#### 谱方法小结

谱方法主要依赖于拉普拉斯矩阵的分解, 它有三种影响:

1. 首先，对图的任何扰动都会导致特征基的改变。
2. 其次，学习的过滤器是域相关的，这意味着它们**不能应用于具有不同结构的图**。
3. 第三，特征分解需要$O(N^3)$的计算和$O(N^2)$的内存。

对于谱方法, 通常它们都需要载入整个图到内存中, 来应用图卷积, 这对于处理大的图来说, 不是很有效率.

### 基于空域方法GCN

空域方法定义卷积主要基于节点的空间关系. 为了关联图和图像, 图像的每个像素认为是节点. 每个像素直接连接到它的相邻像素.
