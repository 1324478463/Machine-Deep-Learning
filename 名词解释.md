## epoch

指所有训练数据完成一次前向和后向传递, 与batch大小有关.

## Multilayer Perceptron, MLP

多层感知器, 是一种前向结构的[人工神经网络](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C), 映射一组输入向量到一组输出向量. MLP可以被看作是一个有向图, 由多个的节点层所组成, 每一层都全连接到下一层. 除了输入节点, 每个节点都是一个带有非线性激活函数的神经元（或称处理单元）. 一种被称为[反向传播算法](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)的[监督学习](https://zh.wikipedia.org/wiki/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0)方法常被用来训练MLP. MLP是[感知器](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8)的推广, 克服了感知器不能对[线性不可分](https://zh.wikipedia.org/w/index.php?title=%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86&action=edit&redlink=1)数据进行识别的弱点.

若每个神经元的激活函数都是线性函数, 那么, 任意层数的MLP都可被约简成一个等价的单层[感知器](https://zh.wikipedia.org/wiki/%E6%84%9F%E7%9F%A5%E5%99%A8).

实际上, MLP本身可以使用任何形式的激活函数, 譬如阶梯函数或[逻辑乙形函数](https://zh.wikipedia.org/w/index.php?title=%E9%80%BB%E8%BE%91%E4%B9%99%E5%BD%A2%E5%87%BD%E6%95%B0&action=edit&redlink=1)(logistic sigmoid function), 但为了使用反向传播算法进行有效学习, 激活函数必须限制为[可微函数](https://zh.wikipedia.org/wiki/%E5%8F%AF%E5%BE%AE%E5%87%BD%E6%95%B0). 由于具有良好可微性, 很多[S函数](https://zh.wikipedia.org/wiki/S%E5%87%BD%E6%95%B0), 尤其是[双曲正切函数](https://zh.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E6%AD%A3%E5%88%87%E5%87%BD%E6%95%B0)(Hyperbolic tangent)及[逻辑函数](https://zh.wikipedia.org/wiki/%E9%80%BB%E8%BE%91%E5%87%BD%E6%95%B0), 被采用为激活函数.

## 网络的深度/宽度

卷积核的种类(也就是宽度)对应了网络的宽度, 卷积层数对应了网络的深度. 这两者对网络的拟合都有影响. 但是在现代深度学习中, 大家普遍认为深度比广度的影响更加高.

## rank of tensor

tensorflow 使用一种叫 tensor 的数据结构去展示所有的数据, 我们可以把 tensor 看成是 n 维的 array 或者 list. 在 tensorflow 的各部分图形间流动传递的只能是tensor.

rank 就是 tensor 的维数.
 例如我们所说的标量(Scalar)：
 `s = 8` 维数为 0, 所以它的 rank 为 0.

例如矢量(Vector)：
 `v = [1, 2, 3]`, rank 为 1.

> https://www.jianshu.com/p/9463340683c9

## dense prediction

> https://www.cnblogs.com/fanhaha/p/7810419.html

标注出图像中每个像素点的对象类别, 要求不但给出具体目标的位置, 还要描绘物体的边界, 如图像分割、语义分割、边缘检测等等.

**基于深度学习主要的做法有两种：**

- 基于图像分块：利用像素、超像素块周围小邻域进行独立的分类. (在分类网络中使用全连接层, 固定图像块尺寸)
- 基于全卷积网络：对图像进行pixel-to-pixel 的预测, 可以得到任意大小的图像分割结果, 而且不需要对每个图像块进行分类, 速度快. 重要的两点：**卷积层上采样、skip connection结构**

## patchwise training

> https://medium.com/stanford-ai-for-healthcare/dont-just-scan-this-deep-learning-techniques-for-mri-52610e9b7a85
>
> ![img](https://cdn-images-1.medium.com/max/1600/1*PYvWGBgZvPVTmFlJGEy5zw.png)
>
> > The goal of semantic segmentation is to predict a class for each pixel. The architecture above shows the “patch-wise” approach ([Ciresan et al. 2012](https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf)).
>
> While a CNN for classification outputs the probability of the entire image belonging to each class in question, a CNN for segmentation assigns a label to each pixel (or “voxel” if the image is 3D). **An early approach to segmentation was to run patches of pixels, centered around the pixel of interest, through a CNN classifier ([Ciresan et al. 2012](https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf)).** Doing this for each pixel produces the segmented image. Following [Akkus et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5537095/) (2017), **we will call this “patch-wise” segmentation.** Although relatively simple, this approach is currently one of the most common for MRI segmentation tasks.

## end-to-end

- 参考资料：

- - [知乎讨论：什么叫end-to-end learning?](https://www.zhihu.com/question/50454339)
    - [知乎讨论：什么是 end-to-end 神经网络？](https://www.zhihu.com/question/51435499)

- FCN文章中多次强调, FCN是一个end-to-end的.

- end-to-end概念：输入是原始数据, 输出是想要的最终结果.

- 举例：

- - R-CNN不是end-to-end, 因为还要单独进行region proposal.
    - Faster R-CNN是end-to-end, 因为region proposal也包含在系统中.
    - YOLO是end-to-end.

## 转置卷积

- 有些翻译叫“反卷积”(deconvolution), 但不合适. “转置卷积”(transposed convolution)这个翻译更恰当.

- 在FCN的最后一层, 使用转置卷积结构进行dense prediction.

- FCN论文中的解释：卷基的forward、backward操作对调, 就是转置卷积.

- 即转置卷积的forward就是普通卷积的backward, 转置卷积的backward就是普通卷积的forward.

- 对于转置卷积有两种理解：

- 卷积与转置卷积的可视化计算过程：参考[这里](https://link.zhihu.com/?target=https%3A//github.com/vdumoulin/conv_arithmetic).
- 卷积与转置卷积的本质就是进行了矩阵运算, 从矩阵角度理解, 参考[这里](https://link.zhihu.com/?target=http%3A//deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html%23transposed-convolution-arithmetic).
- 参考资料：

  - [Github: Convolution arithmetic(各种卷积、反卷积示意图, 非常推荐)](https://link.zhihu.com/?target=https%3A//github.com/vdumoulin/conv_arithmetic)
  - [Transposed convolution arithmetic](https://link.zhihu.com/?target=http%3A//deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html%23transposed-convolution-arithmetic)
  - [知乎文章：反卷积层/转置卷积层--transposed convolution arithmetic](https://zhuanlan.zhihu.com/p/27099017)
  - [知乎提问：如何理解深度学习中的deconvolution networks？](https://www.zhihu.com/question/43609045)
  - [知乎提问：怎样通俗易懂地解释反卷积？](https://www.zhihu.com/question/48279880)
