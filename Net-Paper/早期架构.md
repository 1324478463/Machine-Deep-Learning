## 早期架构

### lenet5架构

是一个开创性的工作，因为图像的特征是分布在整个图像当中的，并且学习参数利用卷积在相同参数的多个位置中提取相似特性的一种有效方法。回归到1998年当时没有GPU来帮助训练，甚至CPU速度都非常慢。因此，对比使用每个像素作为一个单独的输入的多层神经网络，Lenet5能够节省参数和计算是一个关键的优势。lenet5论文中提到，全卷积不应该被放在第一层，因为图像中有着高度的空间相关性，并利用图像各个像素作为单独的输入特征不会利用这些相关性。因此有了CNN的三个特性了：1.局部感知、2.下采样、3.权值共享。

![img](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/lenet5.jpg)

**LeNet5小结：**

* 卷积神经网络使用3层架构：卷积、下采样、非线性激活函数
* 使用卷积提取图像空间特征
* 下采样使用了图像平均稀疏性
* 激活函数采用了tanh或者sigmoid函数
* 多层神经网络（MLP）作为最后的分类器
* 层之间使用稀疏连接矩阵，以避免大的计算成本

总的来说LeNet5架构把人们带入深度学习领域，值得致敬。从2010年开始近几年的神经网络架构大多数都是基于LeNet的三大特性。

### Dan Ciresan Net

2010年Dan Claudiu Ciresan和Jurgen Schmidhuber发表了一个[GPU神经网络](http://arxiv.org/abs/1003.0358)。论文里面证明了使用 NVIDIA GTX 280 GPU之后能够处理高达9层的神经网络。

从此之后，Nvidia公司的股价开始不断攀升，深度学习也越来越为人们所熟知。