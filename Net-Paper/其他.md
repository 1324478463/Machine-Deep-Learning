## VGG

2014年是个绽放年，出了两篇重要的论文：VGG、GoogLeNet。

来自牛津大学的[VGG](http://arxiv.org/abs/1409.1556)网络是第一个在每个卷积层使用更小的3×3卷积核对图像进行卷积，并把这些小的卷积核排列起来作为一个卷积序列。通俗点来讲就是对原始图像进行3×3卷积，然后再进行3×3卷积，连续使用小的卷积核对图像进行多次卷积。

或许很多人看到这里也很困惑为什么使用那么小的卷积核对图像进行卷积，并且还是使用连续的小卷积核？VGG一开始提出的时候刚好与LeNet的设计原则相违背，因为LeNet相信大的卷积核能够捕获图像当中相似的特征（权值共享）。AlexNet在浅层网络开始的时候也是使用9×9、11×11卷积核，并且尽量在浅层网络的时候避免使用1×1的卷积核。但是VGG的神奇之处就是在于使用多个3×3卷积核可以模仿较大卷积核那样对图像进行局部感知。后来多个小的卷积核串联这一思想被GoogleNet和ResNet等吸收。

从下图我们可以看出来，VGG使用多个3x3卷积来对高维特征进行提取。因为如果使用较大的卷积核，参数就会大量地增加、运算时间也会成倍的提升。例如3x3的卷积核只有9个权值参数，使用7*7的卷积核权值参数就会增加到49个。因为缺乏一个模型去对大量的参数进行归一化、约减，或者说是限制大规模的参数出现，因此训练核数更大的卷积网络就变得非常困难了。

VGG相信如果使用大的卷积核将会造成很大的时间浪费，减少的卷积核能够减少参数，节省运算开销。虽然训练的时间变长了，但是总体来说预测的时间和参数都是减少的了。

![img](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/vgg.jpg)

## GoogLeNet

2014年，在google工作的Christian Szegedy为了找到一个深度神经网络结构能够有效地减少计算资源，于是有了这个[GoogleNet](https://arxiv.org/abs/1409.4842)了（也叫做Inception V1）。

从2014年尾到现在，深度学习模型在图像内容分类方面和视频分类方面有了极大的应用。在这之前，很多一开始对深度学习和神经网络都保持怀疑态度的人，现在都涌入深度学习的这个领域，理由很简单，因为深度学习不再是海市蜃楼，而是变得越来越接地气。就连google等互联网巨头都已经在深度学习领域布局，成立了各种各样的人工智能实验室。

Christian在思考如何才能够减少深度神经网络的计算量，同时获得比较好的性能的框架。即使不能两全其美，退而求其次能够保持在相同的计算成本下，能够有更好的性能提升这样的框架也行。于是后面Christian和他的team在google想出了这个模型：

![img](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/inceptionv1.jpg)

其乍一看基本上是1×1,3×3和5×5卷积核的并行合并。但是，最重要的是使用了1×1卷积核（NiN）来减少后续并行操作的特征数量。这个思想现在叫做“bottleneck layer”。

## Bottleneck layer

受NiN的启发，googleNet的Bottleneck layer减少了特征的数量，从而减少了每一层的操作复杂度，因此可以加快推理时间。在将数据传递到下一层卷积之前，特征的数量减少了4左右。因此这种设计架构因为大量节省计算成本而名声大噪。

让我们详细研究一下这个Bottleneck layer。假设输入时256个feature map进来，256个feature map输出，假设Inception层只执行3x3的卷积，那么这就需要这行 (256x256) x (3x3) 次卷积左右（大约589,000次计算操作）。再假设这次589,000次计算操作在google的服务器上面用了0.5ms的时间，计算开销还是很大的。现在Bottleneck layer的思想是先来减少特征的数量，我们首先执行256 -> 64 1×1卷积，然后在所有Bottleneck layer的分支上对64大小的feature map进行卷积，最后再64 -> 256 1x1卷积。 操作量是：

* 256×64 × 1×1 = 16,000s
* 64×64 × 3×3 = 36,000s
* 64×256 × 1×1 = 16,000s

总共约70,000，而我们以前有近600,000。几乎减少10倍的操作！

虽然我们做的操作较少，但我们并没有失去这一层特征。实际上，Bottleneck layer已经在ImageNet数据集上表现非常出色，并且也将在稍后的架构例如ResNet中使用到。

成功的原因是输入特征是相关的，因此可以通过适当地与1x1卷积组合来去除冗余。然后，在卷积具有较少数目的特征之后，它们可以再次扩展并作用于下一层输入。

## Inception V3

Christian的团队确实很厉害，2015年2月他们又发表了新的文章关于在googleNet中加入一个[Batch-normalized](http://arxiv.org/abs/1502.03167)层。Batch-normalized层归一化计算图层输出处所有特征图的平均值和标准差，并使用这些值对其响应进行归一化。这对应于“白化”数据非常有效，并且使得所有神经层具有相同范围并且具有零均值的响应。这有助于训练，因为下一层不必学习输入数据中的偏移，并且可以专注于如何最好地组合特征。

2015年12月，他们发布了一个新版本的GoogLeNet([Inception V3](http://arxiv.org/abs/1512.00567))模块和相应的架构，并且更好地解释了原来的GoogLeNet架构，GoogLeNet原始思想：

* 通过构建平衡深度和宽度的网络，最大化网络的信息流。在进入pooling层之前增加feature maps
* 当网络层数深度增加时，特征的数量或层的宽度也相对应地增加
* 在每一层使用宽度增加以增加下一层之前的特征的组合
* 只使用3x3卷积

因此最后的模型就变成这样了：

![img](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/inceptionv3.jpg)

网络架构最后还是跟GoogleNet一样使用pooling层+softmax层作为最后的分类器。

## ResNet

2015年12月[ResNet](https://arxiv.org/pdf/1512.03385v1.pdf)发表了，时间上大概与Inception v3网络一起发表的。其中ResNet的一个重要的思想是：输出的是两个连续的卷积层，并且输入时绕到下一层去。这句话不太好理解可以看下图。

![img](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/resnetb.jpg)

但在这里，他们绕过两层，并且大规模地在网络中应用这中模型。在2层之后绕过是一个关键，因为绕过单层的话实践上表明并没有太多的帮助，然而绕过2层可以看做是在网络中的一个小分类器！看上去好像没什么感觉，但这是很致命的一种架构，因为通过这种架构最后实现了神经网络超过1000层。傻了吧，之前我们使用LeNet只是5层，AlexNet也最多不过7层。

![img](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/resnetbottleneck.jpg)

该层首先使用1x1卷积然后输出原来特征数的1/4，然后使用3×3的卷积核，然后再次使用1x1的卷积核但是这次输出的特征数为原来输入的大小。如果原来输入的是256个特征，输出的也是256个特征，但是这样就像Bottleneck Layer那样说的大量地减少了计算量，但是却保留了丰富的高维特征信息。

ResNet一开始的时候是使用一个7x7大小的卷积核，然后跟一个pooling层。当然啦，最后的分类器跟GoogleNet一样是一个pooling层加上一个softmax作为分类器。下图左边是VGG19拥有190万个参数，右图是34层的ResNet只需要36万个参数：

[![img](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/ResNet.jpg)](https://chenzomi12.github.io/2016/12/13/CNN-Architectures/ResNet.jpg)

**ResNet网络特征**

* ResNet可以被看作并行和串行多个模块的结合
* ResNet上部分的输入和输出一样，所以看上去有点像RNN，因此可以看做是一个更好的生物神经网络的模型