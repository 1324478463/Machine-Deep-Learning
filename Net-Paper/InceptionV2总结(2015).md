# Inception V2

## 前言

2015年2月Google又发表了新的文章, 在googLeNet中加入一个[Batch-normalized](http://arxiv.org/abs/1502.03167)层。

Batch-normalized层归一化计算图层输出处所有特征图的平均值和标准差，并使用这些值对其响应进行归一化。这对应于“白化”数据非常有效，并且使得所有神经层具有相同范围并且具有零均值的响应。

这有助于训练，因为下一层不必学习输入数据中的偏移，并且可以专注于如何最好地组合特征。

## 概要

训练深度神经网络的复杂性在于，**每层输入的分布在训练过程中会发生变化**，因为前面的层的参数会发生变化。

通过要求*较低的学习率*和*仔细的参数初始化*减慢了训练，并且使具有饱和非线性的模型训练起来非常困难。我们将这种现象称为*内部协变量转移*，并通过标准化层输入来解决这个问题。

我们的方法力图使标准化成为模型架构的一部分，并为*每个训练小批量数据*执行标准化。

* 批标准化使我们能够使用**更高的学习率**，并且**不用太注意初始化**。
* 它也作为一个**正则化项**，在某些情况下**不需要Dropout**。

将批量标准化应用到最先进的图像分类模型上，批标准化在取得相同的精度的情况下，**减少了14倍的训练步骤**，并以显著的差距击败了原始模型。

## 构思

### 直接使用SGD存在的问题

使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为m的*小批量数据*$x_{1…m}$。通过计算$1/m∑_{m_i=1}∂ℓ(xi,Θ)/∂Θ$，使用小批量数据来近似损失函数关于参数的梯度。

使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。

* 首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而改善。

* 第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算m次效率更高。

> 存在问题: 虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。
>
> 层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。
>
> **当学习系统的输入分布发生变化时，据说会经历 协变量转移（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。**
>
> 然而，协变量漂移的概念可以扩展到整个学习系统之外，应用到学习系统的一部分，例如子网络或一层。考虑网络计算$ ℓ=F_2(F_1(u,Θ_1),Θ_2)$,  $F_1$和$F_2$是任意变换, 学习参数 $Θ_1，Θ_2$, 以便最小化损失$ℓ$。
>
> *学习$Θ_2$可以看作输入$x=F_1(u,Θ_1)$送入到子网络$ℓ=F_2(x,Θ_2)​$。*



## 新意

## 实践

## 代码