# Inception V2

## 前言

2015年2月Google又发表了新的文章, 在googLeNet中加入一个[Batch-normalized](http://arxiv.org/abs/1502.03167)层。

Batch-normalized层归一化计算图层输出处所有特征图的平均值和标准差，并使用这些值对其响应进行归一化。这对应于“白化”数据非常有效，并且使得所有神经层具有相同范围并且具有零均值的响应。

这有助于训练，因为下一层不必学习输入数据中的偏移，并且可以专注于如何最好地组合特征。

## 新意

训练深度神经网络的复杂性在于，**每层输入的分布在训练过程中会发生变化**，因为前面的层的参数会发生变化。

通过要求*较低的学习率*和*仔细的参数初始化*减慢了训练，并且使具有饱和非线性的模型训练起来非常困难。我们将这种现象称为*内部协变量转移*，并通过标准化层输入来解决这个问题。

我们的方法力图使标准化成为模型架构的一部分，并为*每个训练小批量数据*执行标准化。

* 批标准化使我们能够使用**更高的学习率**，并且**不用太注意初始化**。
* 它也作为一个**正则化项**，在某些情况下**不需要Dropout**。

将批量标准化应用到最先进的图像分类模型上，批标准化在取得相同的精度的情况下，**减少了14倍的训练步骤**，并以显著的差距击败了原始模型。

## 构思

### 直接使用SGD存在的问题

使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为m的*小批量数据*$x_{1…m}$。通过计算$1/m∑_{m_i=1}∂ℓ(xi,Θ)/∂Θ$，使用小批量数据来近似损失函数关于参数的梯度。

使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。

* 首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而改善。

* 第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算m次效率更高。

> 存在问题: 虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。
>
> 训练的复杂性在于**每层的输入受到前面所有层的参数的影响**——因此当网络变得更深时，网络参数的微小变化就会被放大。
>
> 层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。

**当学习系统的输入分布发生变化时，据说会经历 协变量转移（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。**

然而，协变量漂移的概念可以扩展到整个学习系统之外，应用到学习系统的一部分，例如子网络或一层。

### 解决的方向

考虑网络计算$ ℓ=F_2(F_1(u,Θ_1),Θ_2)$,  $F_1$和$F_2$是任意变换, 学习参数 $Θ_1，Θ_2$, 以便最小化损失$ℓ$。

*学习$Θ_2$可以看作输入$x=F_1(u,Θ_1)$送入到子网络$ℓ=F_2(x,Θ_2)$。*

例如，梯度下降步骤
$$
Θ_2←Θ_2−α/m∑_{i=1m}∂F_2(x_i,Θ_2)/∂Θ_2
$$
（对于批大小m和学习率α）与输入为x的单独网络$F_2$完全等价。

因此，输入分布特性使训练更有效——例如训练数据和测试数据之间有相同的分布——也适用于训练子网络。

> 考虑固定输入分布

### 固定输入分布好处

1. 因此**x的分布在时间上保持固定是有利**的。然后，$Θ_2$不必重新调整来补偿x分布的变化。

2. 子网络输入的固定分布对于子网络外的层也有积极的影响。

   > 考虑一个激活函数为$g(x)=1/(1+exp(−x))$的层，u是层输入，权重矩阵W和偏置向量b是要学习的层参数，$g(x)=1/(1+exp(−x))$。
   >
   > 随着$|x|$的增加，$g′(x)$趋向于0。这意味着对于$x=Wu+b$的所有维度，除了那些具有小的绝对值之外，流向u的**梯度将会消失**，模型将缓慢的进行训练。
   >
   > 然而，由于**x受W,b和下面所有层的参数的影响**，训练期间那些参数的改变可能会将x的许多维度移动到非线性的饱和状态并减慢收敛。这个影响随着网络深度的增加而放大。
   >
   > 在实践中，饱和问题和由此产生的梯度消失通常通过使用**修正线性单元(Nair & Hinton, 2010) $ReLU(x)=max(x,0)$，仔细的初始化(Bengio & Glorot, 2010; Saxe et al., 2013)和小的学习率**来解决。
   >
   > 然而，如果我们能**保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速**。

### 解决方案--BN

由于训练过程中网络参数的变化，我们将*内部协变量转移*定义为网络激活分布的变化。消除它可以保证更快的训练。为了改善训练，我们寻求减少内部协变量转移的方法。固定层输入x的分布, 就是最直接的思路.

随着训练的进行，通过固定层输入x的分布，我们期望提高训练速度。众所周知(LeCun et al., 1998b; Wiesler & Ney, 2011)**如果对网络的输入进行白化，网络训练将会收敛的更快**——即输入线性变换为具有零均值和单位方差，并去相关。

**通过白化每一层的输入，我们将采取措施实现输入的固定分布，消除内部协变量转移的不良影响。**

我们提出了一种新的机制，我们称为**批标准化**，它是减少内部协变量转移的一个步骤，这样做可以显著加速深度神经网络的训练。

它通过标准化步骤来实现，标准化步骤**修正了层输入的均值和方差**。

* 批标准化**减少了梯度对参数或其初始值尺度上的依赖**, 对通过网络的梯度流动有好的影响, 这允许我们使用更高的学习率而没有发散的风险。
* 批标准化**使模型正则化**并减少了对Dropout(Srivastava et al., 2014)的需求。

* 批标准化通过**阻止网络陷入饱和模式**让使用饱和非线性成为可能。

### 实际尝试

#### 修改网络或根据网络激活值来更改优化方法的参数

我们考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数(Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins & Kavukcuoglu)。

然而，如果这些修改分散在优化步骤中，那么梯度下降步骤可能会试图**以要求标准化进行更新的方式来更新参数**，这会降低梯度下降步骤的影响力。

> 例如，考虑一个层，其输入u加上学习到的偏置b，通过减去在训练集上计算的激活值的均值对结果进行归一化：$\hat{x} = x−E[x]，x=u+b, X=x_{1…N}$是训练集上x值的集合，$E[x]=1/N∑^N_{i=1}x_i$。
>
> 如果梯度下降步骤忽略了$E[x]$对b的依赖，那它将更新$b←b+Δb$，其中$Δb∝−∂ℓ/∂\hat x$。
>
> 然后$u+(b+Δb)−E[u+(b+Δb)]=u+b−E[u+b]$。
>
> 因此，结合**b的更新**和接下来**标准化中的改变**会导致层的输出没有变化(原本带标准化的前向输出就是$u+(b+Δb)−E[u+(b+Δb)]=u+b−E[u+b]$)，从而导致损失没有变化。
>
> 随着训练的继续，*b将无限增长而损失保持不变*。
>
> 如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。
>
> > :question:
> >
> > 这里指的是对u的处理? 为什么会更糟?
>
> 我们在最初的实验中已经观察到了这一点，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。

上述方法的问题是梯度下降优化没有考虑到标准化中发生的事实。

为了解决这个问题，我们希望确保对于任何参数值，网络*总是*产生具有所需分布的激活值。这样做将允许关于模型参数损失的梯度来解释标准化，以及它对模型参数Θ的依赖。

设x为层的输入，将其看作向量，X是这些输入在训练集上的集合。标准化可以写为变换$\hat x=Norm(x,X)$ . 它不仅依赖于给定的训练样本x而且依赖于所有样本X——它们中的每一个都依赖于Θ，如果x是由另一层生成的。





## 实践

## 代码