# 梯度下降与梯度上升

通常, 您使用梯度上升来最大化似然函数, 并使用梯度下降来最小化成本函数. 梯度下降和上升几乎都是相同的. 让我举一个具体的例子, 使用一个简单的基于梯度的优化友好算法和一个凹/凸 似然/成本函数：逻辑回归.

您希望在逻辑回归中最大化的似然函数是![在此处输入图像描述](https://i.stack.imgur.com/vRpFZ.png).

其中“phi”只是sigmoid函数![在此处输入图像描述](https://i.stack.imgur.com/BGoE9.png)

现在, 你想要一个用于梯度上升的concav函数：

![在此处输入图像描述](https://i.stack.imgur.com/VCrcH.png)

类似地, 您可以将其写为反向, 以获得可通过梯度下降最小化的成本函数.

![在此处输入图像描述](https://i.stack.imgur.com/hj0WT.png)

对于对数似然, 您将导出并应用梯度上升, 如下所示：

![在此处输入图像描述](https://i.stack.imgur.com/6lpk9.png)

![在此处输入图像描述](https://i.stack.imgur.com/SPD75.png)

既然你想同时更新所有权重, 那就让我们把它写成

![在此处输入图像描述](https://i.stack.imgur.com/x6Hn6.png)

现在, 应该很明显看到梯度下降更新与上升更新相同, 但请记住, 我们正在将其制定为“向成本函数的梯度的相反方向迈出一步”

![在此处输入图像描述](https://i.stack.imgur.com/j9xHh.png)
