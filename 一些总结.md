

# 为什么我们要使用深度网络呢？

使用深度网络最主要的优势在于，它能以更加紧凑简洁的方式来表达比浅层网络大得多的函数集合。正式点说，我们可以找到一些函数，这些函数可以用 ![\textstyle k](http://ufldl.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 层网络简洁地表达出来（这里的简洁是指隐层单元的数目只需与输入单元数目呈多项式关系）。但是对于一个只有 ![\textstyle k-1 ](http://ufldl.stanford.edu/wiki/images/math/c/f/a/cfa33ecd624c1213e41d077b9b93980a.png) 层的网络而言，除非它使用与输入单元数目呈指数关系的隐层单元数目，否则不能简洁表达这些函数。

举一个简单的例子，比如我们打算构建一个布尔网络来计算 ![\textstyle n](http://ufldl.stanford.edu/wiki/images/math/0/c/5/0c59de0fa75c1baa1c024aabfa43b2e3.png) 个输入比特的奇偶校验码（或者进行异或运算）。假设网络中的每一个节点都可以进行逻辑“或”运算（或者“与非”运算），亦或者逻辑“与”运算。如果我们拥有一个仅仅由一个输入层、一个隐层以及一个输出层构成的网络，那么该奇偶校验函数所需要的节点数目与输入层的规模 ![\textstyle n](http://ufldl.stanford.edu/wiki/images/math/0/c/5/0c59de0fa75c1baa1c024aabfa43b2e3.png) 呈指数关系( $n^{n}$ )。但是，如果我们构建一个更深点的网络，那么这个网络的规模就可做到仅仅是 ![\textstyle n](http://ufldl.stanford.edu/wiki/images/math/0/c/5/0c59de0fa75c1baa1c024aabfa43b2e3.png) 的多项式函数，可以多层逐级处理。

当处理对象是图像时，我们能够使用深度网络学习到“部分-整体”的分解关系。例如，第一层可以学习如何将图像中的像素组合在一起来检测边缘（正如我们在前面的练习中做的那样）。第二层可以将边缘组合起来检测更长的轮廓或者简单的“目标的部件”。在更深的层次上，可以将这些轮廓进一步组合起来以检测更为复杂的特征。

最后要提的一点是，大脑皮层同样是分多层进行计算的。例如视觉图像在人脑中是分多个阶段进行处理的，首先是进入大脑皮层的“V1”区，然后紧跟着进入大脑皮层“V2”区，以此类推。

梯度下降法（以及相关的L-BFGS算法等）在使用随机初始化权重的深度网络上效果不好的技术原因是：梯度会变得非常小。具体而言，当使用反向传播方法计算导数的时候，随着网络的深度的增加，反向传播的梯度（从输出层到网络的最初几层）的幅度值会急剧地减小。

> 注意，梯度和导数基本相关的概念

结果就造成了整体的损失函数相对于最初几层的权重的导数非常小。这样，当使用梯度下降法的时候，最初几层的权重变化非常缓慢，以至于它们不能够从样本中进行有效的学习。这种问题通常被称为“梯度的弥散”.

# 栈式自编码神经网络

是一个由多层稀疏自编码器组成的神经网络，其前一层自编码器的输出作为其后一层自编码器的输入。对于一个 ![\textstyle n](http://ufldl.stanford.edu/wiki/images/math/0/c/5/0c59de0fa75c1baa1c024aabfa43b2e3.png) 层栈式自编码神经网络，我们沿用自编码器一章的各种符号，假定用 ![\textstyle W^{(k, 1)}, W^{(k, 2)}, b^{(k, 1)}, b^{(k, 2)}](http://ufldl.stanford.edu/wiki/images/math/7/3/c/73c91abb05fbef0c2731db418c090600.png) 表示第 ![\textstyle k](http://ufldl.stanford.edu/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个自编码器对应的 ![\textstyle W^{(1)}, W^{(2)}, b^{(1)}, b^{(2)}](http://ufldl.stanford.edu/wiki/images/math/3/c/9/3c93474c7682f6a4856939d4fa193bc6.png) 参数，那么该栈式自编码神经网络的编码过程就是，按照从前向后的顺序执行每一层自编码器的编码步骤： ![ \begin{align} a^{(l)} = f(z^{(l)}) \\ z^{(l + 1)} = W^{(l, 1)}a^{(l)} + b^{(l, 1)} \end{align} ](http://ufldl.stanford.edu/wiki/images/math/c/4/5/c45be23c8a9c2d2836fa9c559b2e5254.png)  同理，栈式神经网络的解码过程就是，按照从后向前的顺序执行每一层自编码器的解码步骤：  ![ \begin{align} a^{(n + l)} = f(z^{(n + l)}) \\ z^{(n + l + 1)} = W^{(n - l, 2)}a^{(n + l)} + b^{(n - l, 2)} \end{align} ](http://ufldl.stanford.edu/wiki/images/math/b/5/0/b502d47bfac781f8d16290436d891ddb.png)  其中，![\textstyle a^{(n)}](http://ufldl.stanford.edu/wiki/images/math/e/1/d/e1d8e6d013579f217c6a25d87d7ee531.png) 是最深层隐藏单元的激活值，其包含了我们感兴趣的信息，这个向量也是对输入值的更高阶的表示。 通过将 ![\textstyle a^{(n)}](http://ufldl.stanford.edu/wiki/images/math/e/1/d/e1d8e6d013579f217c6a25d87d7ee531.png) 作为softmax分类器的输入特征，可以将栈式自编码神经网络中学到的特征用于分类问题。 

一种比较好的获取栈式自编码神经网络参数的方法是采用逐层贪婪训练法进行训练。即先利用原始输入来训练网络的第一层，得到其参数 ![\textstyle W^{(1,1)}, W^{(1,2)}, b^{(1,1)}, b^{(1,2)}](http://ufldl.stanford.edu/wiki/images/math/4/2/f/42fe8477d1ab7e4090f01b1caa5e6cdb.png)；然后网络第一层将原始输入转化成为由隐藏单元激活值组成的向量（假设该向量为A），接着把A作为第二层的输入，继续训练得到第二层的参数 ![\textstyle W^{(2,1)}, W^{(2,2)}, b^{(2,1)}, b^{(2,2)}](http://ufldl.stanford.edu/wiki/images/math/6/e/6/6e630937a176c48a27ba40f4656b23cc.png)；最后，对后面的各层同样采用的策略，即将前层的输出作为下一层输入的方式依次训练。

对于上述训练方式，在训练每一层参数的时候，会固定其它各层参数保持不变。所以，如果想得到更好的结果，在上述预训练过程完成之后，可以通过反向传播算法同时调整所有层的参数以改善结果，这个过程一般被称作“微调（fine-tuning）”。 

实际上，使用逐层贪婪训练方法将参数训练到快要收敛时，应该使用微调。反之，如果直接在随机化的初始权重上使用微调，那么会得到不好的结果，因为参数会收敛到局部最优。 

如果你只对以分类为目的的微调感兴趣，那么惯用的做法是丢掉栈式自编码网络的“解码”层，直接把最后一个隐藏层的 ![\textstyle a^{(n)}](http://ufldl.stanford.edu/wiki/images/math/e/1/d/e1d8e6d013579f217c6a25d87d7ee531.png) 作为特征输入到softmax分类器进行分类，这样，分类器（softmax）的分类错误的梯度值就可以直接反向传播给编码层了。 

> 那解码层有什么价值？

栈式自编码神经网络具有强大的表达能力及深度神经网络的所有优点。 

更进一步，它通常能够获取到输入的“层次型分组”或者“部分-整体分解”结构。为了弄清这一点，回顾一下，自编码器倾向于学习得到能更好地表示输入数据的特征。因此，栈式自编码神经网络的第一层会学习得到原始输入的一阶特征（比如图片里的边缘），第二层会学习得到二阶特征，该特征对应一阶特征里包含的一些模式（比如在构成轮廓或者角点时，什么样的边缘会共现）。栈式自编码神经网络的更高层还会学到更高阶的特征。

 举个例子，如果网络的输入数据是图像，网络的第一层会学习如何去识别边，第二层一般会学习如何去组合边，从而构成轮廓、角等。更高层会学习如何去组合更形象且有意义的特征。例如，如果输入数据集包含人脸图像，更高层会学习如何识别或组合眼睛、鼻子、嘴等人脸器官。 

# 线性解码器

回想一下，输出层神经元计算公式如下： 

![ \begin{align} z^{(3)} &= W^{(2)} a^{(2)} + b^{(2)} \\ a^{(3)} &= f(z^{(3)}) \end{align} ](http://ufldl.stanford.edu/wiki/images/math/9/5/7/9570514e4c49fb8fe34abba34b0700b1.png) 

其中 *a*(3) 是输出. 在自编码器中, *a*(3) 近似重构了输入 *x* = *a*(1)。 

 S 型激励函数输出范围是 [0,1]，当 *f*(*z*(3)) 采用该激励函数时，就要对输入限制或缩放，使其位于 [0,1] 范围中。一些数据集，比如 MNIST，能方便将输出缩放到 [0,1] 中，但是很难满足对输入值的要求。比如， PCA 白化处理的输入并不满足 [0,1] 范围要求，也不清楚是否有最好的办法可以将数据缩放到特定范围中。 

设定 *a*(3) = *z*(3) 可以很简单的解决上述问题。从形式上来看，就是输出端使用恒等函数 *f*(*z*) = *z* 作为激励函数，于是有 *a*(3) = *f*(*z*(3)) = *z*(3)。我们称该特殊的激励函数为 **线性激励函数** （称为恒等激励函数可能更好些）。我们仅在输出层中使用线性激励函数。一个 S 型或 tanh 隐含层以及线性输出层构成的自编码器，我们称为**线性解码器**。在输出层激励函数为 *f*(*z*) = *z*, 这样 *f*'(*z*) = 1，所以在计算单元误差的时候，公式有所简化。

# 卷积

[![Convolution schematic.gif](http://ufldl.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)](http://ufldl.stanford.edu/wiki/index.php/File:Convolution_schematic.gif) 

假设给定了 ![r \times c](http://ufldl.stanford.edu/wiki/images/math/f/5/b/f5b34ce727a51879b69d50dbb38cec68.png) 的大尺寸图像，将其定义为 $x_{large}$。首先通过从大尺寸图像中抽取的 ![a \times b](http://ufldl.stanford.edu/wiki/images/math/2/d/1/2d1dc88200d501549f9d6edae3d6c195.png) 的小尺寸图像样本$x_{small}$ 训练稀疏自编码，计算 *f* = σ(*W*(1) $x_{small}$ + *b*(1))（σ 是一个 sigmoid 型函数）得到了 *k* 个特征， 其中 *W*(1) 和 *b*(1) 是可视层单元和隐含单元之间的权重和偏差值。对于每一个 ![a \times b](http://ufldl.stanford.edu/wiki/images/math/2/d/1/2d1dc88200d501549f9d6edae3d6c195.png) 大小的小图像 *x~s~*，计算出对应的值 f~s~ = σ(*W*(1)x~s~ + *b*(1))，对这些 $f_{convolved}$ 值做卷积，就可以得到 ![k \times (r - a + 1) \times (c - b + 1)](http://ufldl.stanford.edu/wiki/images/math/a/5/a/a5ac162e7a320af96172ebc954efc3d3.png) 个卷积后的特征的矩阵。 

96X96的图像将会被分成（96-8+1）X（96-8+1）,即89X89块，每一块的大小是8X8。将这些分出来的块输入到已经训练好的网路中，输入是：64(89X89个样本)，W1是100X64，W1输入，则隐含层的输出是100（89X89个集合），这也是教程中所说的将得到100个集合，每个集合中含有89X89特征的含义所在。就是对抽取的区域逐个运行训练过的稀疏自编码来得到特征的激活值。在这个例子里，显然可以得到 100 个集合，每个集合含有 89x89 个卷积特征。

