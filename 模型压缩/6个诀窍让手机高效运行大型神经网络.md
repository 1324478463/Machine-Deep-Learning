# 让神经网络更小更快的方法

> <https://zhuanlan.zhihu.com/p/45072374>

基本上，我们对三个指标很感兴趣：模型的准确率，运行速度和在手机中所占空间大小。

因为天下没有免费的午餐，所以我们必须要做出一些取舍。

对于大部分方法，我们会着眼于这三个指标，寻找满足我们需求的“饱和点”（saturation point）。所谓“饱和点”就是一个指标中的收益无法再通过其它指标中的损失来实现。在达到“饱和点”之前保持优化值，我们就可以在这两类指标上取得最佳结果。

* 避免全连接层: 使用很少或不使用全连接层，可缩减模型的大小，也能保持很高的准确度。这样既能提高速度，也能减少磁盘空间使用。
* 缩减通道数量和内核大小: 更小的感受域/内核尺寸耗费更少的计算力，但传递的信息也更少. 我们可以用卷积操作中的感受域（receptive field）来做同样的事情。缩减内核大小后，虽然卷积对局部特征的感受度下降，但涉及的参数数量也会减少。这两种情况下，我们通过寻找“饱和点”来选择特征图谱数量/内核大小，这样以来模型的准确度也不会大幅下降。
* 优化降采样: 对于固定数量的层，和固定数量的池化操作，神经网络的表现会大不相同。这是因为数据的表示以及计算负荷取决于池化操作所处的时间点：如果很早执行池化操作，数据的维度就会降低。更少的维度意味着神经网络的处理速度更快，但也意味着更少的信息和更低的准确度。如果很晚才执行池化操作，就会保存大部分信息，模型就有很高的准确度。然而，这也意味着计算对象有很多维度，需要耗费大量计算力。在神经网络中均匀进行降采样是一种非常高效的结构，而且能在模型准确度和运算速度之间取得良好的平衡。也就是我们所说的“饱和点”。早期池化模型会很快。晚期池化模型会更准，均匀池化则兼得两者优点
* 剪切权重: 移除最弱的连接来节省计算时间和空间. 剪切权重就是我们将那些影响量级最小的连接完全移除的过程，这样我们就可以略过关于它们的计算。这样做虽然会降低模型的准确度，但是能让模型更轻便、运行更快。我们需要找到一个“饱和点”，移除尽可能多的连接，但又不会导致准确度大幅下降。
* 将权重量化: 如果想让神经网络的内存占用量尽可能的小，一个方法就是通过量化权重来降低它们的精度。在这个过程中，我们会修改数字的表示使其不再取用任意值，而是一些值的子集。这能让我们只需存储一次经过量化处理的值，然后将它们参考为神经网络的权重。我们通过再次寻找“饱和点”来确定使用多少值。值越多，准确度就越高，但也意味着更大的数字表示。
* 编码模型的表示: 应对权重分布不均的问题。模型的权重量化以后，我们并没有相同数量的权重来支撑每个量化后的值。这意味着在模型的表示中，某些索引的出现频率相对更高，我们可以充分利用这一点！
* 修正准确度损失: 使用我们上面列举的方法后，我们几乎“大修”了手头的神经网络：移除了弱连接（剪切权重），甚至修改了权重（量化操作）。这样能让神经网络超级轻便，运行飞快，但准确度也会发生变化。为了修正这个问题，我们需要在每一步迭代地重新训练神经网络，意思就是在剪切或量化权重后，我们再次训练神经网络，这样它就能适应变化，不断重复这个过程直到权重不再大幅变化为止。
