#  神经网络

## 快速简介

在不诉诸大脑的类比的情况下，依然是可以对神经网络算法进行介绍的。在线性分类一节中，在给出图像的情况下，是使用![s=Wx](https://www.zhihu.com/equation?tex=s%3DWx)来计算不同视觉类别的评分，其中![W](https://www.zhihu.com/equation?tex=W)是一个矩阵，![x](https://www.zhihu.com/equation?tex=x)是一个输入列向量，它包含了图像的全部像素数据。在使用数据库CIFAR-10的案例中，![x](https://www.zhihu.com/equation?tex=x)是一个[3072x1]的列向量，![W](https://www.zhihu.com/equation?tex=W)是一个[10x3072]的矩阵，所以输出的评分是一个包含10个分类评分的向量。

神经网络算法则不同，它的计算公式是![s=W_2max(0,W_1x)](https://www.zhihu.com/equation?tex=s%3DW_2max%280%2CW_1x%29)。其中![W_1](https://www.zhihu.com/equation?tex=W_1)的含义是这样的：举个例子来说，它可以是一个[100x3072]的矩阵，其作用是将图像转化为一个100维的过渡向量。**函数![max(0,-)](https://www.zhihu.com/equation?tex=max%280%2C-%29)是非线性的，它会作用到每个元素。**这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。最终，矩阵![W_2](https://www.zhihu.com/equation?tex=W_2)的尺寸是[10x100]，因此将得到10个数字，这10个数字可以解释为是分类的评分。注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是*改变*的关键点。参数![W_1,W_2](https://www.zhihu.com/equation?tex=W_1%2CW_2)将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。

一个三层的神经网络可以类比地看做![s=W_3max(0,W_2max(0,W_1x))](https://www.zhihu.com/equation?tex=s%3DW_3max%280%2CW_2max%280%2CW_1x%29%29)，其中![W_1,W_2,W_3](https://www.zhihu.com/equation?tex=W_1%2CW_2%2CW_3)是需要进行学习的参数。中间隐层的尺寸是网络的超参数，后续将学习如何设置它们。现在让我们先从神经元或者网络的角度理解上述计算。

## 单个神经元建模

神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，因为神经网络毕竟是从这里得到了启发。

## 生物动机与连接

大脑的基本计算单位是**神经元（neuron）**。人类的神经系统中大约有860亿个神经元，它们被大约10^14-10^15个**突触（synapses）**连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的**树突**获得输入信号，然后沿着它唯一的**轴突（axon）**产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。

在神经元的计算模型中，**沿着轴突传播的信号（比如![x_0](https://www.zhihu.com/equation?tex=x_0)）将基于突触的突触强度（比如![w_0](https://www.zhihu.com/equation?tex=w_0)），与其他神经元的树突进行乘法交互（比如![w_0x_0](https://www.zhihu.com/equation?tex=w_0x_0)）。**

其观点是，突触的强度（也就是权重![w](https://www.zhihu.com/equation?tex=w)），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。

在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会*激活*，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个*速率编码*的观点，将神经元的激活率建模为**激活函数（activation function）![f](https://www.zhihu.com/equation?tex=f)**，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用**sigmoid函数![\sigma](https://www.zhihu.com/equation?tex=%5Csigma)**，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。

![img](assets/d0cbce2f2654b8e70fe201fec2982c7d_hd.jpg)

左边是生物神经元，右边是数学模型。

一个神经元前向传播的实例代码如下：

```python
class Neuron(object):
  # ... 
  def forward(inputs):
    """ 假设输入和权重是1-D的numpy数组，偏差是一个数字 """
    cell_body_sum = np.sum(inputs * self.weights) + self.bias
    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid激活函数
    return firing_rate
```

换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数![\sigma(x)=1/(1+e^{-x})](https://www.zhihu.com/equation?tex=%5Csigma%28x%29%3D1%2F%281%2Be%5E%7B-x%7D%29)。在本节的末尾部分将介绍不同激活函数的细节。

**粗糙模型**：要注意这个对于生物神经元的建模是非常粗糙的：在实际中，有很多不同类型的神经元，每种都有不同的属性。生物神经元的树突可以进行复杂的非线性计算。突触并不就是一个简单的权重，它们是复杂的非线性动态系统。很多系统中，输出的峰值信号的精确时间点非常重要，说明速率编码的近似是不够全面的。鉴于所有这些已经介绍和更多未介绍的简化，如果你画出人类大脑和神经网络之间的类比，有神经科学背景的人对你的板书起哄也是非常自然的。如果你对此感兴趣，可以看看这份[评论](https://link.zhihu.com/?target=https%3A//physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf)或者最新的[另一份](https://link.zhihu.com/?target=http%3A//www.sciencedirect.com/science/article/pii/S0959438814000130)。

神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力“喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）**输入空间中的某些线性区域**。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。

**二分类Softmax分类器**。举例来说，可以把![\displaystyle\sigma(\Sigma_iw_ix_i+b)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Csigma%28%5CSigma_iw_ix_i%2Bb%29)看做其中一个分类的概率![P(y_i=1|x_i;w)](https://www.zhihu.com/equation?tex=P%28y_i%3D1%7Cx_i%3Bw%29)，其他分类的概率为![P(y_i=0|x_i;w)=1-P(y_i=1|x_i;w)](https://www.zhihu.com/equation?tex=P%28y_i%3D0%7Cx_i%3Bw%29%3D1-P%28y_i%3D1%7Cx_i%3Bw%29)，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分类一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。

**二分类SVM分类器**。或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。

**理解正则化**。在SVM/Softmax的例子中，正则化损失从生物学角度可以看做*逐渐遗忘*，因为它的效果是让所有突触权重![w](https://www.zhihu.com/equation?tex=w)在参数更新过程中逐渐向着0变化。

> 不断的学习过程就是对于一类知识不断强化其特征不断巩固的过程

## 常用激活函数

每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数：

---



![img](https://pic3.zhimg.com/80/677187e96671a4cac9c95352743b3806_hd.jpg)

左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。

### Sigmoid

sigmoid非线性函数的数学公式是![\displaystyle\sigma(x)=1/(1+e^{-x})](https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Csigma%28x%29%3D1%2F%281%2Be%5E%7B-x%7D%29)，函数图像如上图的左边所示。在前一节中已经提到过，它输入实数值并将其“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（**saturated**）的激活（1）。然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：

* *Sigmoid函数饱和使梯度消失*。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘（链式法则）。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，**几乎就没有信号通过神经元传到权重再到数据了**。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，<u>如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了</u>。
* *Sigmoid函数的输出不是零中心的*。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果**输入神经元的数据总是正数（比如在![f=w^Tx+b](https://www.zhihu.com/equation?tex=f%3Dw%5ETx%2Bb)中每个元素都![x>0](https://www.zhihu.com/equation?tex=x%3E0)），那么关于![w](https://www.zhihu.com/equation?tex=w)的梯度（结果和x有关）在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式![f](https://www.zhihu.com/equation?tex=f)而定）。**这将会导致梯度下降权重更新时出现z字型的下降。然而，<u>可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题</u>。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。

### Tanh

tanh非线性函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，*tanh非线性函数比sigmoid非线性函数更受欢迎*。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：![tanh(x)=2\sigma(2x)-1](https://www.zhihu.com/equation?tex=tanh%28x%29%3D2%5Csigma%282x%29-1)。

---

![img](assets/83682a138f6224230f5b0292d9c01bd2_hd.jpg)

左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当![x=0](https://www.zhihu.com/equation?tex=x%3D0)时函数值为0。当![x>0](https://www.zhihu.com/equation?tex=x%3E0)函数的斜率为1。右边是从 [Krizhevsky](https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf)等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。

###  ReLU

在近些年ReLU变得非常流行。它的函数公式是![f(x)=max(0,x)](https://www.zhihu.com/equation?tex=f%28x%29%3Dmax%280%2Cx%29)。换句话说，这个激活函数就是一个关于0的阈值（如上图左侧）。使用ReLU有以下一些优缺点：

* 优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（ [Krizhevsky ](https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf)等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。
* 优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。
* 缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通**过合理设置学习率，这种情况的发生概率会降低。**

> 怎么理解诶？

### Leaky ReLU

Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x<0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。所以其函数公式为![f(x)=1(x<0)(\alpha x)+1(x>=0)(x)](https://www.zhihu.com/equation?tex=f%28x%29%3D1%28x%3C0%29%28%5Calpha+x%29%2B1%28x%3E%3D0%29%28x%29)其中![\alpha](https://www.zhihu.com/equation?tex=%5Calpha)是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。

Kaiming He等人在2015年发布的论文[Delving Deep into Rectifiers](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.01852)中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。

### Maxout

一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用![f(w^Tx+b)](https://www.zhihu.com/equation?tex=f%28w%5ETx%2Bb%29)函数形式。一个相关的流行选择是Maxout（最近由[Goodfellow](https://link.zhihu.com/?target=http%3A//www-etud.iro.umontreal.ca/%7Egoodfeli/maxout.html)等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：![max(w^T_1x+b_1,w^T_2x+b_2)](https://www.zhihu.com/equation?tex=max%28w%5ET_1x%2Bb_1%2Cw%5ET_2x%2Bb_2%29)。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当![w_1,b_1=0](https://www.zhihu.com/equation?tex=w_1%2Cb_1%3D0)的时候）。

* 这样Maxout神经元就拥有ReLU单元的所有优点（**线性操作和不饱和**），而没有它的缺点（**死亡的ReLU单元**）。
* 然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致**整体参数的数量激增**。

---

以上就是一些常用的神经元及其激活函数。最后需要注意一点：<u>在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。</u>

**一句话**：“*那么该用那种呢？*”

**用ReLU非线性函数。注意设置好学习率**，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。

## 神经网络结构

### 灵活地组织层

**将神经网络算法以神经元的形式图形化。**神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。也就是说，一些神经元的输出是另一些神经元的输入。在网络中是不允许循环的，因为这样会导致前向传播的无限循环。通常神经网络模型中神经元是分层的，而不是像生物神经元一样聚合成大小不一的团状。对于普通神经网络，最普通的层的类型是**全连接层（fully-connected layer）**。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。下面是两个神经网络的图例，都使用的全连接层：（图中没有画出偏置单元）

![img](assets/ccb56c1fb267bc632d6d88459eb14ace_hd.jpg)

左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。

### 命名规则

**当我们说N层神经网络的时候，我们没有把输入层算入。**因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说逻辑回归或者SVM只是单层神经网络的一个特例。研究者们也会使用*人工神经网络（Artificial Neural Networks 缩写ANN）*或者*多层感知器（Multi-Layer Perceptrons 缩写MLP）*来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，他们更倾向于用*单元（unit）*而不是神经元作为术语。

**输出层。**和神经网络中其他层不同，**输出层的神经元一般是不会有激活函数的（或者也可以认为它们有一个线性相等的激活函数）**。这是因为最后的输出层<u>大多用于表示分类评分值</u>，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）。

> 如之前的代码：
>
> ![1536646710484](assets/1536646710484.png)

**确定网络尺寸。**用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数，用上面图示的两个网络举例：

![img](assets/ccb56c1fb267bc632d6d88459eb14ace_hd.jpg)

* 第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置权重，共26个可学习的参数。
* 第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置权重，共41个可学习的参数。

为了方便对比，现代卷积神经网络能包含约1亿个参数，可由10-20层构成（这就是深度学习）。然而，*有效（effective）*连接的个数因为**参数共享**的缘故大大增多。在后面的卷积神经网络内容中我们将学习更多。

## 前向传播计算举例

*不断重复的矩阵乘法与激活函数交织*。将神经网络组织成层状的一个主要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和高效。用上面那个3层神经网络举例，输入是[3x1]的向量。一个层所有连接的强度可以存在一个单独的矩阵中。

![img](assets/ccb56c1fb267bc632d6d88459eb14ace_hd.jpg)

比如第一个隐层的权重**W1**是[4x3]，所有单元的偏置储存在**b1**中，尺寸[4x1]。这样，每个神经元的权重都在**W1**的一个行中，于是矩阵乘法`np.dot(W1, x)`就能计算该层中所有神经元的激活数据。类似的，**W2**将会是[4x4]矩阵，存储着第二个隐层的连接，**W3**是[1x4]的矩阵，用于输出层。完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。

```python
# 一个3层神经网络的前向传播:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # 激活函数(用的sigmoid)
x = np.random.randn(3, 1) # 含3个数字的随机输入向量(3x1)
h1 = f(np.dot(W1, x) + b1) # 计算第一个隐层的激活数据(4x1)
h2 = f(np.dot(W2, h1) + b2) # 计算第二个隐层的激活数据(4x1)
out = np.dot(W3, h2) + b3 # 神经元输出(1x1)
```

在上面的代码中，**W1，W2，W3，b1，b2，b3**都是网络中可以学习的参数。注意**x**并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是**x**中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络**最后一层通常是没有激活函数**的（例如，在分类任务中它给出一个实数值的分类评分）。

> 全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。

## 表达能力

理解具有全连接层的神经网络的一个方式是：可以认为它们定义了一个由一系列函数组成的函数族，网络的权重就是每个函数的参数。如此产生的问题是：该函数族的表达能力如何？存在不能被神经网络表达的函数吗？

现在看来，拥有至少一个隐层的神经网络是一个*通用的近似器*。在研究（例如1989年的论文[Approximation by Superpositions of Sigmoidal Function](https://link.zhihu.com/?target=http%3A//www.dartmouth.edu/%257Egvc/Cybenko_MCSS.pdf)，或者[Michael Nielsen](https://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap4.html)的这个直观解释。）中已经证明，给出任意连续函数![f(x)](https://www.zhihu.com/equation?tex=f%28x%29)和任意![\epsilon >0](https://www.zhihu.com/equation?tex=%5Cepsilon+%3E0)，均存在一个至少含1个隐层的神经网络![g(x)](https://www.zhihu.com/equation?tex=g%28x%29)（并且网络中有合理选择的非线性激活函数，比如sigmoid），对于![\forall x](https://www.zhihu.com/equation?tex=%5Cforall+x)，使得![|f(x)-g(x)|<\epsilon](https://www.zhihu.com/equation?tex=%7Cf%28x%29-g%28x%29%7C%3C%5Cepsilon)。换句话说，神经网络可以近似任何连续函数。

**既然一个隐层就能近似任何函数，那为什么还要构建更多层来将网络做得更深？**

答案是：虽然一个2层网络在数学理论上能完美地近似所有连续函数，但在实际操作中效果相对较差。在一个维度上，虽然以![a,b,c](https://www.zhihu.com/equation?tex=a%2Cb%2Cc)为参数向量“指示块之和”函数![g(x)=\sum_ic_i1(a_i<x<b_i) ](https://www.zhihu.com/equation?tex=g%28x%29%3D%5Csum_ic_i1%28a_i%3Cx%3Cb_i%29+)也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为它们**表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合**。同时，网络通过最优化算法（例如梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。

另外，在实践中3层的神经网络会比2层的表现好，然而继续加深（做到4，5，6层）很少有太大帮助。

卷积神经网络的情况却不同，在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因素（比如数十(以10为量级)个可学习的层）。对于该现象的一种解释观点是：**因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义**。

全面的研究内容还很多，近期研究的进展也很多。如果你对此感兴趣，我么推荐你阅读下面文献：

* [Deep Learning](https://link.zhihu.com/?target=http%3A//www.deeplearningbook.org/)的[Chapter6.4](https://link.zhihu.com/?target=http%3A//www.deeplearningbook.org/contents/mlp.html)，作者是Bengio等。
* [Do Deep Nets Really Need to be Deep?](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.6184)
* [FitNets: Hints for Thin Deep Nets](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.6550)

## 设置层的数量和尺寸

在面对一个具体问题的时候该确定网络结构呢？到底是不用隐层呢？还是一个隐层？两个隐层或更多？每个层的尺寸该多大？

首先，要知道当我们增加层的数量和尺寸时，网络的容量上升了。即神经元们可以合作表达许多复杂函数，所以表达函数的空间增加。例如，如果有一个在二维平面上的二分类问题。我们可以训练3个不同的神经网络，每个网络都只有一个隐层，但是每层的神经元数目不同：

![img](assets/cf3fc543bf1dc81e2083530a4492b0ec_hd.jpg)

更大的神经网络可以表达更复杂的函数。数据是用不同颜色的圆点表示他们的不同类别，决策边界是由训练过的神经网络做出的。你可以在[ConvNetsJS demo](https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)上练练手。

在上图中，可以看见有更多神经元的神经网络可以表达更复杂的函数。然而这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。<u>**过拟合（Overfitting）**是网络对数据中的噪声有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。</u>举例来说，有20个神经元隐层的网络拟合了所有的训练数据，但是其代价是把决策边界变成了许多不相连的红绿区域。而有3个神经元的模型的表达能力只能用比较宽泛的方式去分类数据。它将数据看做是两个大块，并把个别在绿色区域内的红色点看做噪声。在实际中，这样可以在测试数据中获得更好的**泛化（generalization）**能力。

基于上面的讨论，看起来如果数据不是足够复杂，则似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多方法（**L2正则化，dropout和输入噪音等**），后面会详细讨论。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。

不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：**虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。**因为神经网络是非凸的，就很难从数学上研究这些特性。即便如此，还是有一些文章尝试对这些目标函数进行理解，例如[The Loss Surfaces of Multilayer Networks](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.0233)这篇论文。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。

重申一下，正则化强度是控制神经网络过拟合的好方法。看下图结果：

![img](https://pic3.zhimg.com/80/4f8af027d6059549d160199a1717df14_hd.jpg)

不同正则化强度的效果：每个神经网络都有20个隐层神经元，但是随着正则化强度增加，它的决策边界变得更加平滑。你可以在

ConvNetsJS demo上练练手。

需要记住的是：**不应该因为害怕出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。**

## 小结

* 介绍了生物神经元的粗略模型；
* 讨论了几种不同类型的激活函数，其中ReLU是最佳推荐；
* 介绍了**神经网络**，神经元通过**全连接层**连接，层间神经元两两相连，但是层内神经元不连接；
* 理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算；
* 理解了神经网络是一个**通用函数近似器**，但是该性质与其广泛使用无太大关系。<u>之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够某种程度上做出“正确”假设。</u>
* 讨论了更大网络总是更好的这一事实。<u>然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合</u>。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。

## 参考资料

* 使用Theano的[deeplearning.net tutorial](https://link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/mlp.html)
* [ConvNetJS](https://link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/mlp.html)
* [Michael Nielsen's tutorials](https://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap1.html)

---

## 设置数据和模型

在上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了**评分****函数（score function）**的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来数据预处理

关于数据预处理我们有3个常用的符号，数据矩阵**X**，假设其尺寸是**[N x D]**（**N**是数据样本的数量，**D**是数据的维度）。

**均值减法（****Mean subtraction****）**是预处理最常用的形式。它对数据中每个独立*特征*减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码**X -= np.mean(X, axis=0)**实现。而对于图像，更常用的是对所有像素都减去一个值，可以用**X -= np.mean(X)**实现，也可以在3个颜色通道上分别操作。说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。

## 数据预处理

关于数据预处理我们有3个常用的符号，数据矩阵**X**，假设其尺寸是**[N x D]**（**N**是数据样本的数量，**D**是数据的维度）。

**均值减法（Mean subtraction）**是预处理最常用的形式。它<u>对数据中每个独立*特征*减去平均值，</u>从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码`X -= np.mean(X, axis=0)`实现。而<u>对于图像，更常用的是对所有像素都减去一个值</u>，可以用**X -= np.mean(X)**实现，也可以在3个颜色通道上分别操作。

参考之前"ML与处理.md"文档的内容——

>### 自然灰度图像
>
>==均值消减->PCS/ZCA白化==
>
>灰度图像具有平稳特性，我们通常在第一步对每个数据样本分别做均值消减（即减去直流分量），然后采用 PCA/ZCA 白化处理，其中的 `epsilon` 要**足够大以达到低通滤波**的效果。 
>
>### 彩色图像
>
>==简单缩放->PCA/ZCA白化==
>
>对于彩色图像，<u>色彩通道间并不存在平稳特性</u>。因此我们通常首先对数据进行特征缩放（使像素值位于 [0,1] 区间），然后使用足够大的 `epsilon` 来做 PCA/ZCA。**注意**在进行 PCA 变换前需要对特征进行分量均值归零化。

这里对于彩色图像的处理和之前不同，亦是分通道后使用均值消减。

**归一化（Normalization）**是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。

1. 第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为**X /= np.std(X, axis=0)**。
2. 第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。

![img](assets/e743b6777775b1671c3b5503d7afbbc4_hd.jpg)

一般数据预处理流程：**左边：**原始的2维输入数据。**中间：**在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。**右边：**每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。

**PCA和白化（Whitening）**是另一种预处理形式。在这种处理中，先对数据（各个特征）进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。

```python
# 假设输入数据矩阵X的尺寸为[N x D]
X -= np.mean(X, axis = 0) # 对数据进行零中心化(重要)
cov = np.dot(X.T, X) / X.shape[0] # 得到数据的协方差矩阵
```

数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的*协方差*。具体来说，该矩阵的对角线上的元素是方差。还有，<u>协方差矩阵是对称和[半正定](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Positive-definite_matrix%23Negative-definite.2C_semidefinite_and_indefinite_matrices)的</u>。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。

```python
U,S,V = np.linalg.svd(cov)
```

$U$的列是特征向量，$S$是装有奇异值的1维数组（因为cov是对称且半正定的，所以$S$中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：

```python
Xrot = np.dot(X, U) # 对数据去相关性
```

注意$U$的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。

因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算**Xrot**的协方差矩阵，将会看到它是对角对称的。**np.linalg.svd**的一个良好性质是在它的返回值$U$中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有**方差**的维度。 这个操作也被称为主成分分析（ [Principal Component Analysis](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Principal_component_analysis) 简称PCA）降维：

```python
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced 变成 [N x 100]
```

经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大**方差**的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。

最后一个在实践中会看见的变换是**白化（whitening）**。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。

**该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。**

该操作的代码如下：

```python
# 对数据进行白化操作:
# 除以特征值 
Xwhite = Xrot / np.sqrt(S + 1e-5)
```

*警告：夸大的噪声*。

注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度（导致分母过小）。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。

![img](https://pic1.zhimg.com/80/aae11de6e6a29f50d46b9ea106fbb02a_hd.jpg)

PCA/白化——左边是二维的原始数据。中间：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。右边：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。

我们可以使用CIFAR-10数据将这些变化可视化出来。CIFAR-10训练集的大小是50000x3072，其中每张图片都可以拉伸为3072维的行向量。我们可以计算[3072 x 3072]的协方差矩阵然后进行奇异值分解（比较耗费计算性能），那么经过计算的特征向量看起来是什么样子呢？

![img](assets/8608c06086fc196228f4dda78499a2d9_hd.jpg)

**最左**：一个用于演示的集合，含49张图片。

**左二**：3072个特征值向量中的前144个。靠前面的特征向量解释了数据中大部分的方差，可以看见它们与图像中较低的频率相关。

**第三张**是49张经过了PCA降维处理的图片，展示了144个特征向量。这就是说，展示原始图像是每个图像用3072维的向量，向量中的元素是图片上某个位置的像素在某个颜色通道中的亮度值。而现在每张图片只使用了一个144维的向量，其中每个元素表示了特征向量对于组成这张图片的贡献度。为了让图片能够正常显示，需要将144维度重新变成基于像素基准的3072个数值。因为U是一个旋转，可以通过乘以`U.transpose()[:144,:]`来实现，然后将得到的3072个数值可视化。可以看见图像变得有点模糊了，**这正好说明前面的特征向量获取了较低的频率**。然而，大多数信息还是保留了下来。

**最右**：将“白化”后的数据进行显示。其中144个维度中的方差都被压缩到了相同的数值范围。然后144个白化后的数值通过乘以`U.transpose()[:144,:]`转换到图像像素基准上。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。

> “现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了”这句话的含义？
>
> 为什么较低的频率代表了大多数的方差？又为什么可以忽略不计？
>
> 为什么较高的频率代表相对较少的方差？

**实践操作。**在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。

**常见错误。**进行预处理很重要的一点是：任何预处理策略（比如数据均值）都<u>只能在训练集数据上进行计算</u>，算法训练完毕后再应用到验证集或者测试集上。

例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。**应该怎么做呢？应该先分成训练/验证/测试集，<u>只是从训练集中求图片平均值</u>，然后各个集（训练/验证/测试集）中的图像<u>再减去这个平均值</u>。**

## 权重初始化

我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。

**错误：全零初始化。**让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，<u>如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头</u>。

**小随机数初始化。**因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来*打破对称性*。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。

小随机数权重初始化的实现方法是：`W = 0.01 * np.random.randn(D,H)`。**其中**`randn`**函数是基于零均值和标准差的一个高斯分布**（**译者注：国内教程一般习惯称均值参数为期望![\mu](https://www.zhihu.com/equation?tex=%5Cmu)**）来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。

**警告**。并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。

> 如何理解“梯度与权重值是成比例的”？

**使用`1/sqrt(n)`校准方差**。上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为：`w = np.random.randn(n) / sqrt(n)`。**其中**n是输入数据的数量。

> 除以$\sqrt{n}$怎么理解？见后。
>
> 这是为了让![s](https://www.zhihu.com/equation?tex=s)有和输入![x](https://www.zhihu.com/equation?tex=x)一样的方差。

这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以**提高收敛的速度**。上述结论的推导过程如下：假设权重![w](https://www.zhihu.com/equation?tex=w)和输入![x](https://www.zhihu.com/equation?tex=x)之间的内积为![s=\sum^n_iw_ix_i](https://www.zhihu.com/equation?tex=s%3D%5Csum%5En_iw_ix_i)，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查![s](https://www.zhihu.com/equation?tex=s)的方差：

![\displaystyle Var(assets/equation-1536665512017)=Var(\sum^n_iw_ix_i)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+Var%28s%29%3DVar%28%5Csum%5En_iw_ix_i%29)

![\displaystyle =\sum^n_iVar(assets/equation-1536665533107)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%3D%5Csum%5En_iVar%28w_ix_i%29)

![\displaystyle =\sum^n_i[E(assets/equation-1536665549636)]^2Var(x_i)+E[(x_i)]^2Var(w_i)+Var(xIi)Var(w_i)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%3D%5Csum%5En_i%5BE%28w_i%29%5D%5E2Var%28x_i%29%2BE%5B%28x_i%29%5D%5E2Var%28w_i%29%2BVar%28xIi%29Var%28w_i%29)

> 这里写错了，应该是$Var(x_i)$

![\displaystyle =\sum^n_iVar(assets/equation-1536665606155)Var(w_i)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%3D%5Csum%5En_iVar%28x_i%29Var%28w_i%29)

![\displaystyle =(nVar(w))Var(x)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%3D%28nVar%28w%29%29Var%28x%29)

> 这里主要使用的性质是：
>
> 第二步，因为不同i,j之间的结果不相关（这里应该是独立），所以协方差为0。
>
> ![\operatorname {Var} \left(\sum _{i=1}^{N}X_{i}\right)=\sum _{i,j=1}^{N}\operatorname {Cov} (X_{i},X_{j})=\sum _{i=1}^{N}\operatorname {Var} (X_{i})+\sum _{i\neq j}\operatorname {Cov} (X_{i},X_{j}).](https://wikimedia.org/api/rest_v1/media/math/render/svg/5e613e05d5f933809eee60844cfc60ab5cf7bb74)
>
> ![{\displaystyle \operatorname {Var} \left(\sum _{i=1}^{n}X_{i}\right)=\sum _{i=1}^{n}\operatorname {Var} (X_{i}).}](https://wikimedia.org/api/rest_v1/media/math/render/svg/e68daf39a0d384d9074c3f5137a633137e9bd4fb)
>
> 使用方差优先于其他分散度量的一个原因是不相关随机变量之和（或差异）的方差是它们方差的总和。
>
> 第三步：使用的是独立的情况
>
> * Product of independent variables
>
>   If two variables X and Y are [independent](https://en.wikipedia.org/wiki/Independence_(probability_theory)), the variance of their product is given by
>
>   ![{\begin{aligned}\operatorname {Var} (assets/78ba93b03d6960ae5d2a0bd47d027cd2354c9d36)&=[E(X)]^{2}\operatorname {Var} (Y)+[E(Y)]^{2}\operatorname {Var} (X)+\operatorname {Var} (X)\operatorname {Var} (Y).\end{aligned}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/78ba93b03d6960ae5d2a0bd47d027cd2354c9d36)
>
>   Equivalently, using the basic properties of expectation, it is given by
>
>   ![\operatorname {Var} (assets/e431a2d861b74d8df6334564e9cdf79be994a6f8)=E(X^{2})E(Y^{2})-[E(X)]^{2}[E(Y)]^{2}.](https://wikimedia.org/api/rest_v1/media/math/render/svg/e431a2d861b74d8df6334564e9cdf79be994a6f8)
>
> * Product of statistically dependent variables
>
>   In general, if two variables are statistically dependent, the variance of their product is given by:
>
>   ![{\begin{aligned}\operatorname {Var} (assets/f69eebf38df8850d0a28086083d2791a159b0875)&=E[X^{2}Y^{2}]-[E(XY)]^{2}\\&=\operatorname {Cov} (X^{2},Y^{2})+E(X^{2})E(Y^{2})-[E(XY)]^{2}\\&=\operatorname {Cov} (X^{2},Y^{2})+(\operatorname {Var} (X)+[E(X)]^{2})(\operatorname {Var} (Y)+[E(Y)]^{2})-[\operatorname {Cov} (X,Y)+E(X)E(Y)]^{2}\end{aligned}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f69eebf38df8850d0a28086083d2791a159b0875)

在前两步，使用了[方差的性质](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Variance)。

在第三步，因为假设输入和权重的平均值都是0，所以![E[x_i]=E[w_i]=0](https://www.zhihu.com/equation?tex=E%5Bx_i%5D%3DE%5Bw_i%5D%3D0)。<u>注意这并不是一般化情况，比如在ReLU单元中均值就为正</u>。在最后一步，我们假设所有的![w_i,x_i](https://www.zhihu.com/equation?tex=w_i%2Cx_i)都服从同样的分布。从这个推导过程我们可以看见，**如果想要![s](https://www.zhihu.com/equation?tex=s)有和输入![x](https://www.zhihu.com/equation?tex=x)一样的方差，那么在初始化的时候必须保证每个权重![w](https://www.zhihu.com/equation?tex=w)的方差是![1/n](https://www.zhihu.com/equation?tex=1%2Fn)**。

> 为什么要使s有和输入x一样的方差？

又因为对于一个随机变量![X](https://www.zhihu.com/equation?tex=X)和标量![a](https://www.zhihu.com/equation?tex=a)，有![Var(aX)=a^2Var(X)](https://www.zhihu.com/equation?tex=Var%28aX%29%3Da%5E2Var%28X%29)，这就说明可以基于一个标准高斯分布的`X`($Var(X)=1$)，然后乘以![a=\sqrt{1/n}](https://www.zhihu.com/equation?tex=a%3D%5Csqrt%7B1%2Fn%7D)，也就是运算($aX$)，使其方差为![1/n](https://www.zhihu.com/equation?tex=1%2Fn)，于是得出：`w = np.random.randn(n) / sqrt(n)`。

Glorot等在论文[Understanding the difficulty of training deep feedforward neural networks](https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)中作出了类似的分析。在论文中，作者推荐初始化公式为![ \( \text{Var}(w) = 2/(n_{in} + n_{out}) \) ](https://www.zhihu.com/equation?tex=+%5C%28+%5Ctext%7BVar%7D%28w%29+%3D+2%2F%28n_%7Bin%7D+%2B+n_%7Bout%7D%29+%5C%29+)，其中![\(n_{in}, n_{out}\)](https://www.zhihu.com/equation?tex=%5C%28n_%7Bin%7D%2C+n_%7Bout%7D%5C%29)是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。

该主题下最新的一篇论文是：[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852)，作者是He等人。文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是![2.0/n](https://www.zhihu.com/equation?tex=2.0%2Fn)。代码为`w = np.random.randn(n) * sqrt(2.0/n)`。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。

**稀疏初始化（Sparse initialization）。**另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元**都同下一层固定数目的神经元随机连接**（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。

> 都说了“将所有权重矩阵设为0”，为什么还有“其权重数值由一个小的高斯分布生成”？

**偏置（biases）的初始化。**通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以<u>通常还是使用0来初始化偏置参数</u>。

**实践。**<u>当前的推荐是使用ReLU激活函数，并且使用`w = np.random.randn(n) * sqrt(2.0/n)`来进行权重初始化</u>，关于这一点，[这篇文章](https://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852)有讨论。

**批量归一化（Batch Normalization）。**[批量归一化](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.03167)是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛，其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为上面的参考文献中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，<u>使用了批量归一化的网络对于不好的初始值有更强的鲁棒性</u>。最后一句话总结：<u>批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起</u>。搞定！

## 正则化 Regularization

有不少方法是通过控制神经网络的容量来防止其过拟合的：

### L2正则化

可能是最常用的正则化方法了。可以通过惩罚目标函数中所有参数的平方将其实现。即对于网络中的每个权重![w](https://www.zhihu.com/equation?tex=w)，向目标函数中增加一个![\frac{1}{2}\lambda w^2](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5Clambda+w%5E2)，其中![\lambda](https://www.zhihu.com/equation?tex=%5Clambda)是正则化强度。前面这个![\frac{1}{2}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D)很常见，是因为加上![\frac{1}{2}](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D)后，该式子关于![w](https://www.zhihu.com/equation?tex=w)梯度就是![\lambda w](https://www.zhihu.com/equation?tex=%5Clambda+w)而不是![2\lambda w](https://www.zhihu.com/equation?tex=2%5Clambda+w)了。

**L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。**

在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以`w += -lambda * W`向着0线性下降。

### L1正则化

是另一个相对常用的正则化方法。对于每个![w](https://www.zhihu.com/equation?tex=w)我们都向目标函数增加一个![\lambda|w|](https://www.zhihu.com/equation?tex=%5Clambda%7Cw%7C)。

L1和L2正则化也可以进行组合：![\lambda_1|w|+\lambda_2w^2](https://www.zhihu.com/equation?tex=%5Clambda_1%7Cw%7C%2B%5Clambda_2w%5E2)，这也被称作[Elastic net regularizaton](https://link.zhihu.com/?target=http%3A//web.stanford.edu/%257Ehastie/Papers/B67.2%2520%25282005%2529%2520301-320%2520Zou%2520%26%2520Hastie.pdf)。

L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，**如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。**

### 最大范式约束（Max norm constraints）

另一种形式的正则化是给每个神经元中权重向量的<u>量级设定上限</u>，并使用<u>投影梯度下降</u>来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量![\overrightarrow{w}](https://www.zhihu.com/equation?tex=%5Coverrightarrow%7Bw%7D)必须满足![||\overrightarrow{w}||_2<c](https://www.zhihu.com/equation?tex=%7C%7C%5Coverrightarrow%7Bw%7D%7C%7C_2%3Cc)这一条件，一般![c](https://www.zhihu.com/equation?tex=c)值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，**即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的**。

### 随机失活（Dropout）

是一个简单又极其有效的正则化方法。该方法由Srivastava在论文[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf)中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数![p](https://www.zhihu.com/equation?tex=p)的概率被激活或者被设置为0。

![img](assets/63fcf4cc655cb04f21a37e86aca333cf_hd.jpg)

图片来源自[论文](https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf)，展示其核心思路。在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都**共享参数**）。

> 如何共享参数？

在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。

#### 普通随机失活

一个3层神经网络的普通版随机失活可以用下面代码实现：

```python
""" 普通版随机失活: 不推荐实现 (看下面笔记) """

p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱，越容易保留神经元

def train_step(X):
  """ X中是输入数据 """

  # 3层neural network的前向传播
  H1 = np.maximum(0, np.dot(W1, X) + b1) # 第一层输出
  U1 = np.random.rand(*H1.shape) < p # 第一个随机失活遮罩，要失活的位置就是0，也就是值大于p的要失活
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2) # 第二层输出
  U2 = np.random.rand(*H2.shape) < p # 第二个随机失活遮罩
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3 # 第三层输出 = 第三层输入
  # 反向传播:计算梯度... (略)
  # 进行参数更新... (略)

def predict(X):
  # 前向传播时模型集成
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p
  out = np.dot(W3, H2) + b3
```

在上面的代码中，**train_step**函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据**X**创建一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩**U1**和**U2**加入进去。

**注意：**在**predict**函数中不进行随机失活，但是对于两个隐层的输出都要乘以![p](https://www.zhihu.com/equation?tex=p)，调整其数值范围。

这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以![p=0.5](https://www.zhihu.com/equation?tex=p%3D0.5)为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。

为了理解这点，先假设有一个神经元![x](https://www.zhihu.com/equation?tex=x)的输出，那么进行随机失活的时候，该神经元的输出就是![px+(1-p)0](https://www.zhihu.com/equation?tex=px%2B%281-p%290)，这是有![1-p](https://www.zhihu.com/equation?tex=1-p)的概率神经元的输出为0。

在测试时神经元总是激活的，就必须调整![x\to px](https://www.zhihu.com/equation?tex=x%5Cto+px)来保持同样的预期输出。

> 为什么在测试时神经元总是激活的？因为不进行随机失活。

在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。

#### 反向随机失活

上述操作不好的性质是必须在测试时对激活数据要按照![p](https://www.zhihu.com/equation?tex=p)进行数值范围调整。既然测试性能如此关键，实际更倾向使用**反向随机失活（inverted dropout）**，它是<u>在训练时就进行数值范围调整</u>，从而让前向传播在测试时保持不变。**这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变**。反向随机失活的代码如下：

```python
""" 
反向随机失活: 推荐实现方式.
在训练的时候drop和调整数值范围，测试时不做任何事.
"""

p = 0.5 # 激活神经元的概率. p值更高 = 随机失活更弱

def train_step(X):
  # 3层neural network的前向传播
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # 第一个随机失活遮罩. 注意/p!
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # 第二个随机失活遮罩. 注意/p!
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3

  # 反向传播:计算梯度... (略)
  # 进行参数更新... (略)

def predict(X):
  # 前向传播时模型集成
  H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```

在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献：

* [Dropout paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) by Srivastava et al. 2014.
* [Dropout Training as Adaptive Regularization](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)：“我们认为：在使用费希尔信息矩阵（[fisher information matrix](https://en.wikipedia.org/wiki/Fisher_information_metric)）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”

#### 其他内容

**前向传播中的噪音**

在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过*分析法*（在使用随机失活的本例中就是乘以![p](https://www.zhihu.com/equation?tex=p)）或*数值法*（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是[DropConnect](http://cs.nyu.edu/~wanli/dropc/)，它在前向传播的时候，一系列权重被随机设置为0。提前说一下，卷积神经网络同样会吸取这类方法的优点，比如随机汇合（stochastic pooling），分级汇合（fractional pooling），数据增长（data augmentation）。我们在后面会详细介绍。

**偏置正则化** 

在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，**因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果**。

> 怎么理解？

然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。

**每层正则化**

对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。

**实践**

* 通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。
* 在使用L2正则化的同时在所有层后面使用随机失活也很常见。

![p](https://www.zhihu.com/equation?tex=p)值一般默认设为0.5，也可能在验证集上调参。

## 损失函数

我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。

损失函数的第二个部分是*数据损失*，它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，![L=\frac{1}{N}\sum_iL_i](https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_iL_i)中，![N](https://www.zhihu.com/equation?tex=N)是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为![f=f(x_i;W)](https://www.zhihu.com/equation?tex=f%3Df%28x_i%3BW%29)，在实际中你可能需要解决以下几类问题：

1. 分类问题

   在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）。在这类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：

   ![\displaystyle L_i=\sum_{j\not=y_i}max(assets/equation-1536737255565)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_%7Bj%5Cnot%3Dy_i%7Dmax%280%2Cf_j-f_%7By_i%7D%2B1%29)

   之前简要提起过，有些学者的论文中指出平方折叶损失（即使用![max(0,f_j-f_{y_i}+1)^2](https://www.zhihu.com/equation?tex=max%280%2Cf_j-f_%7By_i%7D%2B1%29%5E2)）算法的结果会更好。

   第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：

   ![\displaystyle L_i=-log(assets/equation-1536737315300)](https://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_je%5E%7Bf_j%7D%7D%29)

2. 问题：类别数目巨大

    当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用*分层Softmax（**Hierarchical Softmax**）*了（[参考文献](http://arxiv.org/pdf/1310.4546.pdf)）。分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。

   > 类似于霍夫曼树？

3. 属性（Attribute）分类

   上面两个损失公式的前提，都是假设每个样本只有一个正确的标签![y_i](http://www.zhihu.com/equation?tex=y_i)。但是如果![y_i](http://www.zhihu.com/equation?tex=y_i)是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间**并不相互排斥**呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。例如，针对每个分类的二分类器会采用下面的公式：

   ![\displaystyle L_i=\sum_jmax(assets/equation-1536749266216)](http://www.zhihu.com/equation?tex=%5Cdisplaystyle+L_i%3D%5Csum_jmax%280%2C1-y_%7Bij%7Df_j%29)

   上式中，求和是对所有分类![j](http://www.zhihu.com/equation?tex=j)，![y_{ij}](http://www.zhihu.com/equation?tex=y_%7Bij%7D)的值为1或者-1，具体根据**第i个样本是否被第j个属性打标签**而定，当该类别**被正确预测并展示**的时候，分值向量![f_j](http://www.zhihu.com/equation?tex=f_j)为正，其余情况为负。

   可以发现，当一个正样本的得分小于+1，或者一个负样本得分大于-1的时候，算法就会累计损失值。

   另一种方法是对每种属性训练一个独立的逻辑回归分类器。二分类的逻辑回归分类器只有两个分